#!/usr/bin/env python3
"""
ğŸ§  Enterprise Brain Search & Memory System v2.0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ëŒì˜ ì¡°ì ˆ + AI ìë™í™”ê°€ ê²°í•©ëœ ì§€ì‹ ê²°ì • ì‹œìŠ¤í…œ

4ëŒ€ ê°€ì¤‘ì¹˜ (Memory Weights):
  - WR: Recency (ìµœê·¼ì„±) - ìµœì‹  ë¬¸ì„œ ìš°ì„ 
  - WP: Popularity (ì¡°íšŒìˆ˜) - ì‚¬ìš© ë¹ˆë„ ë°˜ì˜
  - WV: Relevance (ìƒê´€ë„) - ì˜ë¯¸ì  ê±°ë¦¬
  - WM: Importance (ì¤‘ìš”ë„) - ì¡°ì§ì  ê°€ì¹˜

ìµœì¢… ì ìˆ˜: Score = WR*R + WP*P + WV*V + WM*M
"""

import json
import re
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("holarchy.brain_engine")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìì—°ì–´ ë¦¬ë·° â†’ ì ìˆ˜ ìë™ ë³´ì • ì‹œìŠ¤í…œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ReviewClassifier:
    """
    ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ë¦¬ë·°ë¥¼ positive/negative/actionableë¡œ ë¶„ë¥˜
    
    ì˜ˆì‹œ:
        "ì¢‹ì•„" â†’ positive
        "í—·ê°ˆë¦¼" â†’ negative  
        "ë‹¤ì‹œ ë´ì•¼í•¨" â†’ actionable
    """
    RULES = {
        "positive": ["ì¢‹", "ì •í™•", "ì¤‘ìš”", "ê¹”ë”", "í•µì‹¬", "ìœ ìš©", "ë§", "í›Œë¥­", "ì™„ë²½", "í•„ìš”"],
        "negative": ["í—·ê°ˆ", "í‹€ë¦¼", "ì• ë§¤", "ë¶ˆí•„ìš”", "ê´€ë ¨ì—†", "ì•½í•¨", "ì´ìƒ", "ì˜¤ë¥˜", "ì˜ëª»", "ì•„ë‹˜"],
        "actionable": ["ë‹¤ì‹œ", "ë³´ì™„", "ì—…ë°ì´íŠ¸", "ìˆ˜ì •", "ì •ë¦¬", "í™•ì¸", "ê²€í† ", "ì¶”ê°€", "ê°œì„ "]
    }
    
    @staticmethod
    def classify(text: str) -> str:
        """ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë¼ë²¨ë¡œ ë¶„ë¥˜"""
        text_lower = text.lower()
        
        # ìš°ì„ ìˆœìœ„: actionable > negative > positive
        for label in ["actionable", "negative", "positive"]:
            keywords = ReviewClassifier.RULES[label]
            if any(k in text_lower for k in keywords):
                return label
        
        return "positive"  # ê¸°ë³¸ê°’


class ReviewAdjuster:
    """
    ë¦¬ë·° ë¼ë²¨ì— ë”°ë¼ ì ìˆ˜ ìë™ ì¡°ì •
    
    - positive: importanceâ†‘ relevanceâ†‘ accuracyâ†‘ authorityâ†‘
    - negative: importanceâ†“ relevanceâ†“ accuracyâ†“
    - actionable: strategic_valueâ†‘ reusabilityâ†‘ importance slightâ†‘
    """
    ADJUST = {
        "positive": {
            "importance": +0.15,
            "relevance": +0.10,
            "accuracy": +1,
            "authority": +1
        },
        "negative": {
            "importance": -0.15,
            "relevance": -0.10,
            "accuracy": -1
        },
        "actionable": {
            "importance": +0.05,
            "reusability": +1,
            "strategic_value": +1
        }
    }
    
    @staticmethod
    def apply(review_label: str, eval_dict: Dict) -> Dict:
        """ì ìˆ˜ ì¡°ì • ì ìš© (0~5 ë²”ìœ„ ìë™ í´ë¨í•‘)"""
        adj = ReviewAdjuster.ADJUST.get(review_label, {})
        updated = eval_dict.copy()
        
        for key, delta in adj.items():
            current = updated.get(key, 0)
            # 0~5 ë²”ìœ„ë¡œ í´ë¨í•‘ (importance/relevanceëŠ” 0~1)
            if key in ["importance", "relevance"]:
                updated[key] = max(0.0, min(1.0, current + delta))
            else:
                updated[key] = max(0, min(5, int(current + delta)))
        
        return updated


class MemoryProfile(Enum):
    """Memory Style Presets - 5ê°€ì§€ ê¸°ì–µ ìŠ¤íƒ€ì¼"""
    FAST_FRESH = "fast_fresh"      # ìµœì‹  ë¬¸ì„œ ì¤‘ì‹¬
    WISDOM = "wisdom"              # ì¤‘ìš”ë„ ì¤‘ì‹¬ (ì˜¤ë˜ë˜ì–´ë„ ì¤‘ìš”í•œ ê²ƒ)
    BALANCED = "balanced"          # ê· í˜• ëª¨ë“œ
    PATTERN_MINING = "pattern"     # LTM í•µì‹¬ ê·œì¹™ íƒìƒ‰
    TREND = "trend"                # ìµœì‹  + ì¡°íšŒìˆ˜ íŠ¸ë Œë“œ


@dataclass
class MemoryWeights:
    """4ëŒ€ ê°€ì¤‘ì¹˜ ì„¤ì •"""
    recency: float = 0.25      # WR: ìµœê·¼ì„±
    popularity: float = 0.25   # WP: ì¡°íšŒìˆ˜
    relevance: float = 0.25    # WV: ìƒê´€ë„
    importance: float = 0.25   # WM: ì¤‘ìš”ë„
    
    # í”„ë¦¬ì…‹ ì •ì˜
    PRESETS = {
        MemoryProfile.FAST_FRESH: {"recency": 0.5, "popularity": 0.1, "relevance": 0.3, "importance": 0.1},
        MemoryProfile.WISDOM: {"recency": 0.1, "popularity": 0.1, "relevance": 0.3, "importance": 0.5},
        MemoryProfile.BALANCED: {"recency": 0.25, "popularity": 0.25, "relevance": 0.25, "importance": 0.25},
        MemoryProfile.PATTERN_MINING: {"recency": 0.0, "popularity": 0.1, "relevance": 0.4, "importance": 0.5},
        MemoryProfile.TREND: {"recency": 0.4, "popularity": 0.4, "relevance": 0.2, "importance": 0.0},
    }
    
    @classmethod
    def from_profile(cls, profile: MemoryProfile) -> 'MemoryWeights':
        """í”„ë¦¬ì…‹ì—ì„œ ê°€ì¤‘ì¹˜ ìƒì„±"""
        preset = cls.PRESETS.get(profile, cls.PRESETS[MemoryProfile.BALANCED])
        return cls(**preset)
    
    def validate(self) -> bool:
        """ê°€ì¤‘ì¹˜ í•©ì´ 1.0ì¸ì§€ í™•ì¸"""
        total = self.recency + self.popularity + self.relevance + self.importance
        return abs(total - 1.0) < 0.01
    
    def normalize(self):
        """ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1.0ì´ ë˜ë„ë¡)"""
        total = self.recency + self.popularity + self.relevance + self.importance
        if total > 0:
            self.recency /= total
            self.popularity /= total
            self.relevance /= total
            self.importance /= total


@dataclass
class DocumentScore:
    """ë¬¸ì„œë³„ 4ì°¨ì› ì ìˆ˜"""
    holon_id: str
    filename: str
    filepath: str
    
    # 4ëŒ€ ì ìˆ˜ (0~1)
    recency_score: float = 0.0      # R: ìµœê·¼ì„± ì ìˆ˜
    popularity_score: float = 0.0   # P: ì¡°íšŒìˆ˜ ì ìˆ˜
    relevance_score: float = 0.0    # V: ìƒê´€ë„ ì ìˆ˜ (ì¿¼ë¦¬ ì˜ì¡´)
    importance_score: float = 0.0   # M: ì¤‘ìš”ë„ ì ìˆ˜
    
    # ë©”íƒ€ë°ì´í„°
    created_at: str = ""
    updated_at: str = ""
    age_days: int = 0
    access_count: int = 0
    doc_type: str = ""
    layer: str = "wm"  # wm, ltm, archive
    
    # ì‚¬ëŒ í‰ê°€ (0~5)
    human_eval: Dict = field(default_factory=lambda: {
        "accuracy": 0,          # ì •í™•ì„±
        "importance": 0,        # ì¡°ì§ì  ì¤‘ìš”ì„±
        "reusability": 0,       # ì¬ì‚¬ìš© ê°€ëŠ¥ì„±
        "authority": 0,         # ì‹ ë¢°ë„
        "strategic_value": 0    # ì „ëµì  ê°€ì¹˜
    })
    
    def calculate_final_score(self, weights: MemoryWeights) -> float:
        """ìµœì¢… ì ìˆ˜ ê³„ì‚°: Score = WR*R + WP*P + WV*V + WM*M"""
        score = (
            weights.recency * self.recency_score +
            weights.popularity * self.popularity_score +
            weights.relevance * self.relevance_score +
            weights.importance * self.importance_score
        )
        return min(1.0, max(0.0, score))
    
    def get_human_eval_avg(self) -> float:
        """ì‚¬ëŒ í‰ê°€ í‰ê·  (0~1 ì •ê·œí™”)"""
        values = list(self.human_eval.values())
        if not values:
            return 0.0
        return sum(values) / (len(values) * 5)  # 5ì  ë§Œì  ì •ê·œí™”


class BrainEngine:
    """Enterprise Brain Search & Memory System"""
    
    # ë©”ëª¨ë¦¬ ë ˆì´ì–´ ì •ì˜
    MEMORY_LAYERS = {
        "wm": {"name": "Working Memory", "max_age_days": 90, "description": "ìµœê·¼ 90ì¼"},
        "ltm": {"name": "Long-term Memory", "max_age_days": 365, "description": "ì••ì¶• ì €ì¥"},
        "archive": {"name": "Archive", "max_age_days": None, "description": "ì›ë³¸ ì €ì¥ì†Œ"}
    }
    
    def __init__(self, base_path: str = None):
        if base_path:
            self.base_path = Path(base_path)
        else:
            self.base_path = Path(__file__).parent
        
        self.holons_path = self.base_path
        self.docs_root = self.base_path.parent
        self.hte_path = self.base_path.parent.parent / "2 Company" / "4 HTE"
        self.reports_path = self.docs_root / "reports"
        
        # ì„¤ì • íŒŒì¼ ê²½ë¡œ
        self.config_path = self.reports_path / "brain_config.json"
        self.access_log_path = self.reports_path / "access_log.json"
        self.eval_path = self.reports_path / "human_evaluations.json"
        
        # í˜„ì¬ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: Balanced)
        self.weights = MemoryWeights()
        self.current_profile = MemoryProfile.BALANCED
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.reports_path.mkdir(parents=True, exist_ok=True)
        
        # ì„¤ì • ë¡œë“œ
        self._load_config()
    
    def _load_config(self):
        """ì„¤ì • ë¡œë“œ"""
        if self.config_path.exists():
            with open(self.config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
                self.weights = MemoryWeights(
                    recency=config.get("weights", {}).get("recency", 0.25),
                    popularity=config.get("weights", {}).get("popularity", 0.25),
                    relevance=config.get("weights", {}).get("relevance", 0.25),
                    importance=config.get("weights", {}).get("importance", 0.25)
                )
                profile_name = config.get("profile", "balanced")
                try:
                    self.current_profile = MemoryProfile(profile_name)
                except ValueError as e:
                    logger.debug(f"MemoryProfile íŒŒì‹± ì‹¤íŒ¨ [{profile_name}]: {e}")
                    self.current_profile = MemoryProfile.BALANCED
    
    def _save_config(self):
        """ì„¤ì • ì €ì¥"""
        config = {
            "profile": self.current_profile.value,
            "weights": {
                "recency": self.weights.recency,
                "popularity": self.weights.popularity,
                "relevance": self.weights.relevance,
                "importance": self.weights.importance
            },
            "updated_at": datetime.now().isoformat()
        }
        with open(self.config_path, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    def set_profile(self, profile: MemoryProfile):
        """í”„ë¦¬ì…‹ ì ìš©"""
        self.current_profile = profile
        self.weights = MemoryWeights.from_profile(profile)
        self._save_config()
    
    def set_weights(self, recency: float, popularity: float, 
                   relevance: float, importance: float):
        """ê°€ì¤‘ì¹˜ ì§ì ‘ ì„¤ì •"""
        self.weights = MemoryWeights(
            recency=recency,
            popularity=popularity,
            relevance=relevance,
            importance=importance
        )
        self.weights.normalize()  # ì •ê·œí™”
        self.current_profile = MemoryProfile.BALANCED  # Custom
        self._save_config()
    
    def _load_access_log(self) -> Dict:
        """ì ‘ê·¼ ê¸°ë¡ ë¡œë“œ"""
        if self.access_log_path.exists():
            with open(self.access_log_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return {"documents": {}, "last_updated": None}
    
    def _save_access_log(self, log: Dict):
        """ì ‘ê·¼ ê¸°ë¡ ì €ì¥"""
        log["last_updated"] = datetime.now().isoformat()
        with open(self.access_log_path, "w", encoding="utf-8") as f:
            json.dump(log, f, ensure_ascii=False, indent=2)
    
    def _load_evaluations(self) -> Dict:
        """ì‚¬ëŒ í‰ê°€ ë¡œë“œ"""
        if self.eval_path.exists():
            with open(self.eval_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return {"evaluations": {}}
    
    def _save_evaluations(self, evals: Dict):
        """ì‚¬ëŒ í‰ê°€ ì €ì¥"""
        evals["updated_at"] = datetime.now().isoformat()
        with open(self.eval_path, "w", encoding="utf-8") as f:
            json.dump(evals, f, ensure_ascii=False, indent=2)
    
    def record_access(self, holon_id: str):
        """ë¬¸ì„œ ì ‘ê·¼ ê¸°ë¡ (ì¡°íšŒìˆ˜ ì¦ê°€)"""
        log = self._load_access_log()
        
        if holon_id not in log["documents"]:
            log["documents"][holon_id] = {"count": 0, "accesses": []}
        
        log["documents"][holon_id]["count"] += 1
        log["documents"][holon_id]["accesses"].append(datetime.now().isoformat())
        
        # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
        log["documents"][holon_id]["accesses"] = log["documents"][holon_id]["accesses"][-100:]
        
        self._save_access_log(log)
    
    def evaluate_document(self, holon_id: str, accuracy: int = 0, importance: int = 0,
                         reusability: int = 0, authority: int = 0, strategic_value: int = 0):
        """ë¬¸ì„œ í‰ê°€ (ì‚¬ëŒì´ 0~5 ì ìˆ˜ ë¶€ì—¬)"""
        evals = self._load_evaluations()
        
        evals["evaluations"][holon_id] = {
            "accuracy": min(5, max(0, accuracy)),
            "importance": min(5, max(0, importance)),
            "reusability": min(5, max(0, reusability)),
            "authority": min(5, max(0, authority)),
            "strategic_value": min(5, max(0, strategic_value)),
            "evaluated_at": datetime.now().isoformat()
        }
        
        self._save_evaluations(evals)
    
    def review_document(self, holon_id: str, review_text: str) -> Dict:
        """
        ğŸ¯ ìì—°ì–´ ë¦¬ë·° ê¸°ë°˜ ìë™ ì ìˆ˜ ë³´ì •
        
        ì‚¬ìš©ìê°€ ê°„ë‹¨íˆ í•œ ì¤„ë§Œ ì…ë ¥í•˜ë©´ ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ:
        - Importance â†‘â†“
        - Relevance â†‘â†“
        - Human Evaluation Score ìë™ ì—…ë°ì´íŠ¸
        - ë¯¸ë˜ ê²€ìƒ‰ ì ìˆ˜ ë°˜ì˜
        - ë¬¸ì„œ ë ˆì´ì–´ ìë™ ì´ë™ (WM â†’ LTM ë“±)
        
        Args:
            holon_id: ë¬¸ì„œ ID
            review_text: ìì—°ì–´ ë¦¬ë·° (ì˜ˆ: "ì¢‹ì•„", "í—·ê°ˆë¦¼", "ë‹¤ì‹œ ë´ì•¼í•¨")
        
        Returns:
            {"label": str, "before": dict, "after": dict}
        """
        # 1) ë¦¬ë·° ë¼ë²¨ë§
        label = ReviewClassifier.classify(review_text)
        
        # 2) ê¸°ì¡´ í‰ê°€ ë¶ˆëŸ¬ì˜¤ê¸°
        evals = self._load_evaluations()
        
        before = evals["evaluations"].get(holon_id, {
            "accuracy": 0,
            "importance": 0,
            "reusability": 0,
            "authority": 0,
            "strategic_value": 0
        })
        
        # 3) ìë™ ë³´ì •
        after = ReviewAdjuster.apply(label, before)
        after["evaluated_at"] = datetime.now().isoformat()
        after["last_review"] = review_text
        after["last_label"] = label
        
        # 4) ì €ì¥
        evals["evaluations"][holon_id] = after
        self._save_evaluations(evals)
        
        # 5) ë¬¸ì„œ ì ‘ê·¼ ê¸°ë¡ ì¦ê°€ (í•™ìŠµ íš¨ê³¼)
        self.record_access(holon_id)
        
        return {
            "label": label,
            "before": before,
            "after": after
        }
    
    def print_review_result(self, result: Dict):
        """ë¦¬ë·° ê²°ê³¼ ì¶œë ¥"""
        label = result["label"]
        before = result["before"]
        after = result["after"]
        
        # ì´ëª¨ì§€ ë§¤í•‘
        emoji_map = {"positive": "ğŸ‘", "negative": "ğŸ‘", "actionable": "ğŸ“"}
        emoji = emoji_map.get(label, "ğŸ“Œ")
        
        print()
        print(f"{emoji} ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ: {label.upper()}")
        print("-" * 40)
        
        # ë³€í™” í‘œì‹œ
        for key in ["accuracy", "importance", "reusability", "authority", "strategic_value"]:
            b = before.get(key, 0)
            a = after.get(key, 0)
            if a != b:
                arrow = "â†‘" if a > b else "â†“"
                print(f"   {key}: {b} â†’ {a} {arrow}")
        
        print()
    
    def _calculate_recency(self, age_days: int) -> float:
        """ìµœê·¼ì„± ì ìˆ˜ ê³„ì‚° (R)"""
        if age_days <= 7:
            return 1.0
        elif age_days <= 30:
            return 0.8
        elif age_days <= 90:
            return 0.5
        elif age_days <= 180:
            return 0.3
        elif age_days <= 365:
            return 0.1
        else:
            return 0.0
    
    def _calculate_popularity(self, holon_id: str) -> float:
        """ì¡°íšŒìˆ˜ ì ìˆ˜ ê³„ì‚° (P)"""
        log = self._load_access_log()
        
        if holon_id not in log["documents"]:
            return 0.0
        
        doc_log = log["documents"][holon_id]
        count = doc_log.get("count", 0)
        
        # ìµœê·¼ 90ì¼ ë‚´ ì ‘ê·¼ íšŸìˆ˜
        recent_accesses = 0
        cutoff = datetime.now() - timedelta(days=90)
        
        for access_time in doc_log.get("accesses", []):
            try:
                access_dt = datetime.fromisoformat(access_time)
                if access_dt > cutoff:
                    recent_accesses += 1
            except ValueError as e:
                logger.debug(f"ì ‘ê·¼ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ [{access_time}]: {e}")
        
        # 10íšŒ ì´ìƒì´ë©´ 1.0
        return min(1.0, recent_accesses / 10.0)
    
    def _calculate_relevance(self, content: str, query: str = "", 
                             holon: Dict = None) -> float:
        """
        ìƒê´€ë„ ì ìˆ˜ ê³„ì‚° (V) - W ê¸°ë°˜ ë²¡í„° ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­ í•˜ì´ë¸Œë¦¬ë“œ
        
        Args:
            content: ë¬¸ì„œ ì „ì²´ ë‚´ìš©
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            holon: Holon JSON (W í•„ë“œ ì¶”ì¶œìš©)
        """
        if not query:
            return 0.5  # ê¸°ë³¸ê°’
        
        # 1. W.will.drive ë²¡í„° ìœ ì‚¬ë„ (ìˆìœ¼ë©´)
        vector_score = 0.0
        if holon:
            w_drive = holon.get("W", {}).get("will", {}).get("drive", "")
            if w_drive:
                try:
                    from _vector_rag import VectorRAGEngine
                    engine = VectorRAGEngine(str(self.base_path))
                    
                    query_embedding = engine._get_embedding(query)
                    doc_embedding = engine._get_embedding(w_drive)
                    
                    vector_score = engine._cosine_similarity(query_embedding, doc_embedding)
                except Exception as e:
                    # ë²¡í„° RAG ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ í´ë°±
                    pass
        
        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ (í´ë°± ë˜ëŠ” ë³´ì™„)
        content_lower = content.lower()
        query_words = query.lower().split()
        
        matched = sum(1 for word in query_words if word in content_lower)
        
        if len(query_words) == 0:
            keyword_score = 0.5
        else:
            keyword_score = min(1.0, matched / len(query_words))
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: ë²¡í„° 70% + í‚¤ì›Œë“œ 30%
        if vector_score > 0:
            return (vector_score * 0.7) + (keyword_score * 0.3)
        else:
            return keyword_score
    
    def _calculate_importance(self, holon: Dict, content: str) -> float:
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (M) - ê¸°ì¡´ M-score ë¡œì§"""
        # ì„±ê³¼ ì˜í–¥ í‚¤ì›Œë“œ
        impact_keywords = ["ë§¤ì¶œ", "ì„±ê³¼", "KPI", "OKR", "ë‹¬ì„±", "ëª©í‘œ", "ìˆ˜ìµ", "ì„±ì¥", "íš¨ìœ¨"]
        # ì‹œìŠ¤í…œ ë°˜ì˜ í‚¤ì›Œë“œ
        system_keywords = ["ê·œì¹™", "ì›ì¹™", "ì •ì±…", "ê¸°ì¤€", "í‘œì¤€", "í”„ë¡œì„¸ìŠ¤", "ì² í•™", "ê°€ì´ë“œ"]
        # ê°ì •ì  ì„íŒ©íŠ¸ í‚¤ì›Œë“œ
        emotional_keywords = ["ìœ„ê¸°", "ì‹¤íŒ¨", "ì„±ê³µ", "í­ì¦", "í­ë½", "í•´ê²°", "ëŒíŒŒ", "í˜ì‹ "]
        
        content_lower = content.lower()
        
        # ì ìˆ˜ ê³„ì‚°
        impact = sum(0.1 for k in impact_keywords if k in content_lower)
        system = sum(0.15 for k in system_keywords if k in content_lower)
        emotional = sum(0.1 for k in emotional_keywords if k in content_lower)
        
        # ì‚¬ëŒ í‰ê°€ ë°˜ì˜
        evals = self._load_evaluations()
        holon_id = holon.get("holon_id", "")
        if holon_id in evals.get("evaluations", {}):
            eval_data = evals["evaluations"][holon_id]
            human_avg = sum(eval_data.get(k, 0) for k in 
                          ["accuracy", "importance", "reusability", "authority", "strategic_value"])
            human_score = human_avg / 25.0  # 25ì  ë§Œì 
            
            # í‚¤ì›Œë“œ ì ìˆ˜ + ì‚¬ëŒ í‰ê°€ ì ìˆ˜ ê²°í•©
            return min(1.0, (impact + system + emotional + human_score) / 2)
        
        return min(1.0, impact + system + emotional)
    
    def _determine_layer(self, age_days: int, importance: float) -> str:
        """ë©”ëª¨ë¦¬ ë ˆì´ì–´ ê²°ì •"""
        if age_days <= 90:
            return "wm"  # Working Memory
        elif importance >= 0.5 or age_days <= 365:
            return "ltm"  # Long-term Memory
        else:
            return "archive"  # Archive
    
    def _parse_holon(self, filepath: Path) -> Optional[Dict]:
        """Holon JSON íŒŒì‹±"""
        try:
            content = filepath.read_text(encoding="utf-8")
            json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
        except (json.JSONDecodeError, FileNotFoundError, UnicodeDecodeError) as e:
            logger.debug(f"Holon íŒŒì‹± ì‹¤íŒ¨ [{filepath.name}]: {e}")
        return None
    
    def analyze_document(self, filepath: Path, query: str = "") -> Optional[DocumentScore]:
        """ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„"""
        if not filepath.exists():
            return None
        
        content = filepath.read_text(encoding="utf-8")
        holon = self._parse_holon(filepath)
        
        if not holon:
            return None
        
        holon_id = holon.get("holon_id", filepath.stem)
        meta = holon.get("meta", {})
        
        # ë‚ ì§œ íŒŒì‹±
        created_at = meta.get("created_at", datetime.now().strftime("%Y-%m-%d"))
        updated_at = meta.get("updated_at", created_at)
        
        try:
            created_dt = datetime.strptime(created_at[:10], "%Y-%m-%d")
            age_days = (datetime.now() - created_dt).days
        except ValueError as e:
            logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ [{created_at}]: {e}")
            age_days = 0
        
        # 4ëŒ€ ì ìˆ˜ ê³„ì‚°
        recency_score = self._calculate_recency(age_days)
        popularity_score = self._calculate_popularity(holon_id)
        relevance_score = self._calculate_relevance(content, query, holon)  # W ê¸°ë°˜ ë²¡í„° ìœ ì‚¬ë„
        importance_score = self._calculate_importance(holon, content)
        
        # ì ‘ê·¼ ê¸°ë¡ ì¹´ìš´íŠ¸
        log = self._load_access_log()
        access_count = log.get("documents", {}).get(holon_id, {}).get("count", 0)
        
        # ë ˆì´ì–´ ê²°ì •
        layer = self._determine_layer(age_days, importance_score)
        
        # ì‚¬ëŒ í‰ê°€ ë¡œë“œ
        evals = self._load_evaluations()
        human_eval = evals.get("evaluations", {}).get(holon_id, {
            "accuracy": 0, "importance": 0, "reusability": 0,
            "authority": 0, "strategic_value": 0
        })
        
        return DocumentScore(
            holon_id=holon_id,
            filename=filepath.name,
            filepath=str(filepath),
            recency_score=recency_score,
            popularity_score=popularity_score,
            relevance_score=relevance_score,
            importance_score=importance_score,
            created_at=created_at,
            updated_at=updated_at,
            age_days=age_days,
            access_count=access_count,
            doc_type=holon.get("type", "unknown"),
            layer=layer,
            human_eval=human_eval
        )
    
    def search(self, query: str = "", limit: int = 20) -> List[Tuple[DocumentScore, float]]:
        """ê²€ìƒ‰ ì‹¤í–‰ - 4ëŒ€ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìˆœìœ„"""
        all_scores = []
        
        # holons í´ë”
        for md_file in self.holons_path.glob("*.md"):
            if md_file.name.startswith("_"):
                continue
            score = self.analyze_document(md_file, query)
            if score:
                all_scores.append(score)
        
        # meetings/decisions/tasks í´ë”
        for folder in ["meetings", "decisions", "tasks"]:
            folder_path = self.docs_root / folder
            if folder_path.exists():
                for md_file in folder_path.glob("*.md"):
                    if md_file.name.startswith("_"):
                        continue
                    score = self.analyze_document(md_file, query)
                    if score:
                        all_scores.append(score)
        
        # HTE ëª¨ë“ˆ í´ë”
        if self.hte_path.exists():
            for md_file in self.hte_path.rglob("HTE_*.md"):
                score = self.analyze_document(md_file, query)
                if score:
                    all_scores.append(score)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        results = []
        for score in all_scores:
            final = score.calculate_final_score(self.weights)
            results.append((score, final))
        
        # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:limit]
    
    def get_by_layer(self, layer: str) -> List[DocumentScore]:
        """íŠ¹ì • ë ˆì´ì–´ì˜ ë¬¸ì„œë“¤"""
        all_docs = self.search(limit=1000)
        return [doc for doc, _ in all_docs if doc.layer == layer]
    
    def print_weights_panel(self):
        """ê°€ì¤‘ì¹˜ ì¡°ì ˆ íŒ¨ë„ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ›ï¸  Memory Tuning Panel - ê°€ì¤‘ì¹˜ ì¡°ì ˆ")
        print("=" * 60)
        print()
        
        def make_bar(value: float) -> str:
            filled = int(value * 20)
            return "â–ˆ" * filled + "â–‘" * (20 - filled)
        
        print(f"  [ìµœê·¼ì„± (Recency)]     {make_bar(self.weights.recency)} {self.weights.recency:.2f}")
        print(f"  [ì¡°íšŒìˆ˜ (Popularity)]  {make_bar(self.weights.popularity)} {self.weights.popularity:.2f}")
        print(f"  [ìƒê´€ë„ (Relevance)]   {make_bar(self.weights.relevance)} {self.weights.relevance:.2f}")
        print(f"  [ì¤‘ìš”ë„ (Importance)]  {make_bar(self.weights.importance)} {self.weights.importance:.2f}")
        print()
        print(f"  ğŸ“‹ í˜„ì¬ í”„ë¡œí•„: {self.current_profile.value}")
        print()
    
    def print_profiles(self):
        """í”„ë¡œí•„ ëª©ë¡ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ¨ Memory Style Presets - ê¸°ì–µ ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹")
        print("=" * 60)
        print()
        
        profiles = [
            (MemoryProfile.FAST_FRESH, "âš¡ Fast & Fresh", "ìµœì‹  ë¬¸ì„œ ì¤‘ì‹¬, ì‹¤ë¬´ ìµœì‹  ì •ë³´"),
            (MemoryProfile.WISDOM, "ğŸ“š Wisdom", "ì¤‘ìš”ë„ ì¤‘ì‹¬, ì˜¤ë˜ë˜ì–´ë„ í•µì‹¬ ë¬¸ì„œ"),
            (MemoryProfile.BALANCED, "âš–ï¸ Balanced", "ëª¨ë“  ìš”ì†Œ ê· í˜•"),
            (MemoryProfile.PATTERN_MINING, "ğŸ” Pattern Mining", "LTM í•µì‹¬ ê·œì¹™ íƒìƒ‰"),
            (MemoryProfile.TREND, "ğŸ“ˆ Trend", "ìµœì‹  + ì¡°íšŒìˆ˜ íŠ¸ë Œë“œ"),
        ]
        
        for profile, name, desc in profiles:
            preset = MemoryWeights.PRESETS[profile]
            active = "â†’" if profile == self.current_profile else " "
            print(f"  {active} {name:20} {desc}")
            print(f"      WR={preset['recency']:.1f}  WP={preset['popularity']:.1f}  "
                  f"WV={preset['relevance']:.1f}  WM={preset['importance']:.1f}")
            print()
    
    def print_search_results(self, query: str = "", limit: int = 15):
        """ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
        print("=" * 70)
        print(f"ğŸ” Enterprise Brain Search")
        if query:
            print(f"   ì¿¼ë¦¬: \"{query}\"")
        print(f"   í”„ë¡œí•„: {self.current_profile.value}")
        print("=" * 70)
        print()
        
        self.print_weights_panel()
        
        results = self.search(query, limit)
        
        if not results:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return
        
        layer_emoji = {"wm": "ğŸŸ ", "ltm": "ğŸŸ¢", "archive": "ğŸ“¦"}
        
        print("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ (ìµœì¢… ì ìˆ˜ ê¸°ì¤€):")
        print("-" * 70)
        print(f"{'ìˆœìœ„':^4} {'ë¬¸ì„œ':^35} {'ì ìˆ˜':^8} {'ë ˆì´ì–´':^8} {'R':^5} {'P':^5} {'V':^5} {'M':^5}")
        print("-" * 70)
        
        for i, (doc, final_score) in enumerate(results, 1):
            emoji = layer_emoji.get(doc.layer, "â“")
            print(f"{i:3}. {doc.filename[:33]:33} {final_score:.3f}   "
                  f"{emoji} {doc.layer:6} "
                  f"{doc.recency_score:.2f} {doc.popularity_score:.2f} "
                  f"{doc.relevance_score:.2f} {doc.importance_score:.2f}")
        
        print()
        print("ğŸ’¡ ì ìˆ˜ í•´ì„:")
        print("   R=ìµœê·¼ì„±, P=ì¡°íšŒìˆ˜, V=ìƒê´€ë„, M=ì¤‘ìš”ë„")
        print(f"   Final = WRÃ—R + WPÃ—P + WVÃ—V + WMÃ—M")
        print()
    
    def print_layer_summary(self):
        """ë©”ëª¨ë¦¬ ë ˆì´ì–´ë³„ ìš”ì•½"""
        print("=" * 60)
        print("ğŸ§  Enterprise Memory Architecture")
        print("=" * 60)
        print()
        
        all_results = self.search(limit=1000)
        
        layer_stats = {"wm": [], "ltm": [], "archive": []}
        for doc, score in all_results:
            layer_stats[doc.layer].append((doc, score))
        
        layer_info = {
            "wm": ("ğŸŸ ", "Working Memory", "ìµœê·¼ 90ì¼"),
            "ltm": ("ğŸŸ¢", "Long-term Memory", "ì••ì¶• ì €ì¥"),
            "archive": ("ğŸ“¦", "Archive", "ì›ë³¸ ì €ì¥ì†Œ")
        }
        
        for layer, (emoji, name, desc) in layer_info.items():
            docs = layer_stats[layer]
            avg_score = sum(s for _, s in docs) / len(docs) if docs else 0
            
            print(f"{emoji} {name} [{desc}]")
            print(f"   ë¬¸ì„œ ìˆ˜: {len(docs)}ê°œ | í‰ê·  ì ìˆ˜: {avg_score:.3f}")
            print("-" * 50)
            
            for doc, score in docs[:5]:
                print(f"   {doc.filename[:35]:35} ì ìˆ˜: {score:.3f}")
            
            if len(docs) > 5:
                print(f"   ... ì™¸ {len(docs) - 5}ê°œ")
            print()
    
    def save_report(self) -> Path:
        """ë¦¬í¬íŠ¸ JSON ì €ì¥"""
        results = self.search(limit=1000)
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "profile": self.current_profile.value,
            "weights": {
                "recency": self.weights.recency,
                "popularity": self.weights.popularity,
                "relevance": self.weights.relevance,
                "importance": self.weights.importance
            },
            "total_documents": len(results),
            "layers": {"wm": 0, "ltm": 0, "archive": 0},
            "documents": []
        }
        
        for doc, final_score in results:
            report["layers"][doc.layer] += 1
            report["documents"].append({
                "holon_id": doc.holon_id,
                "filename": doc.filename,
                "final_score": round(final_score, 4),
                "layer": doc.layer,
                "scores": {
                    "recency": round(doc.recency_score, 3),
                    "popularity": round(doc.popularity_score, 3),
                    "relevance": round(doc.relevance_score, 3),
                    "importance": round(doc.importance_score, 3)
                },
                "human_eval": doc.human_eval,
                "age_days": doc.age_days,
                "access_count": doc.access_count
            })
        
        report_path = self.reports_path / "brain_search_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report_path


def main():
    """í…ŒìŠ¤íŠ¸"""
    engine = BrainEngine()
    engine.print_profiles()
    engine.print_search_results()


if __name__ == "__main__":
    main()

