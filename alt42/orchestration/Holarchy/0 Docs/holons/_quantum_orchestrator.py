#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Quantum Orchestration - Phase 3: QuantumOrchestrator
=====================================================
ê¸°ì¡´ ë£° ì‹œìŠ¤í…œ ìœ„ì˜ Quantum ì¡°ì • ë ˆì´ì–´

ëª©ì :
- ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ë™ì‹œ íŠ¸ë¦¬ê±° ì‹œ ìµœì  í™œì„±í™” ìˆœì„œ ì œì•ˆ
- StateVector + EntanglementMap + HamiltonianEvolution í†µí•©
- ê¸°ì¡´ ì‹œìŠ¤í…œ ê°„ì„­ ì—†ì´ ìƒìœ„ ì¡°ì •ìë¡œ ë™ì‘

ì°¸ì¡°:
- _quantum_persona_mapper.py (Phase 1: StateVector)
- _quantum_entanglement.py (Phase 2: EntanglementMap)
- _quantum_minimal_test.py (ê¸°ì´ˆ ì‹ í˜¸ ë³€í™˜)
"""

from __future__ import print_function, unicode_literals
import sys
import io

# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì • (ì„œë²„ í™˜ê²½ í˜¸í™˜)
if sys.version_info[0] >= 3:
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    elif hasattr(sys.stdout, 'buffer'):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
else:
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

import math
from typing import Dict, List, Tuple, Optional, Set
from enum import Enum
from collections import defaultdict

# Python 3.6 í˜¸í™˜: dataclasses ëŒ€ì‹  ì¼ë°˜ í´ë˜ìŠ¤ ì‚¬ìš©

# Phase 1, 2 ëª¨ë“ˆ ì„í¬íŠ¸
from _quantum_persona_mapper import (
    StateVector,
    PERSONA_TO_STATE,
    get_state_vector,
    calculate_similarity
)
from _quantum_entanglement import (
    ENTANGLEMENT_MAP,
    get_correlation,
    get_entangled_agents,
    calculate_entanglement_strength,
    get_agent_name,
    AGENT_NAME_TO_ID,
    AGENT_ID_TO_NAME
)

# Phase 7 Data Interface ì„í¬íŠ¸
try:
    from _quantum_data_interface import (
        StandardFeatures,
        DimensionReducer,
        QuantumDataCollector
    )
    DATA_INTERFACE_AVAILABLE = True
except ImportError:
    DATA_INTERFACE_AVAILABLE = False
    StandardFeatures = None
    DimensionReducer = None
    QuantumDataCollector = None


# ==============================================================================
# 1.5. New 8D StateVector í†µí•© (Phase 8)
# ==============================================================================

# New 8D ì°¨ì›ëª… ì •ì˜
NEW_8D_DIMENSIONS = [
    'cognitive_clarity',       # 0: ì¸ì§€ì  ëª…í™•ì„±
    'emotional_stability',     # 1: ì •ì„œì  ì•ˆì •ì„±
    'engagement_level',        # 2: ì°¸ì—¬ ìˆ˜ì¤€
    'concept_mastery',         # 3: ê°œë… ìˆ™ë‹¬ë„
    'routine_strength',        # 4: ë£¨í‹´ ê°•ë„
    'metacognitive_awareness', # 5: ë©”íƒ€ì¸ì§€ ì¸ì‹
    'dropout_risk',            # 6: ì´íƒˆ ìœ„í—˜ë„
    'intervention_readiness'   # 7: ê°œì… ì¤€ë¹„ë„
]


class New8DStateVector:
    """
    Phase 7 Data Interface ê¸°ë°˜ 8D StateVector

    _quantum_data_interface.pyì—ì„œ ìƒì„±ëœ 8D ë²¡í„°ë¥¼ ë˜í•‘
    """

    def __init__(
        self,
        cognitive_clarity=0.5,
        emotional_stability=0.5,
        engagement_level=0.5,
        concept_mastery=0.5,
        routine_strength=0.5,
        metacognitive_awareness=0.5,
        dropout_risk=0.5,
        intervention_readiness=0.5
    ):
        self.cognitive_clarity = cognitive_clarity
        self.emotional_stability = emotional_stability
        self.engagement_level = engagement_level
        self.concept_mastery = concept_mastery
        self.routine_strength = routine_strength
        self.metacognitive_awareness = metacognitive_awareness
        self.dropout_risk = dropout_risk
        self.intervention_readiness = intervention_readiness

    @classmethod
    def from_list(cls, values):
        """8D ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒì„±"""
        if len(values) != 8:
            raise ValueError(f"Expected 8 values, got {len(values)}")
        return cls(
            cognitive_clarity=values[0],
            emotional_stability=values[1],
            engagement_level=values[2],
            concept_mastery=values[3],
            routine_strength=values[4],
            metacognitive_awareness=values[5],
            dropout_risk=values[6],
            intervention_readiness=values[7]
        )

    @classmethod
    def from_agent_data(cls, student_id, agent_contexts):
        """
        ì—ì´ì „íŠ¸ ë°ì´í„°ì—ì„œ ì§ì ‘ ìƒì„± (Phase 7 í†µí•©)

        Args:
            student_id: í•™ìƒ ID
            agent_contexts: {agent_id: {key: value}} í˜•íƒœì˜ ì—ì´ì „íŠ¸ ë°ì´í„°

        Returns:
            New8DStateVector ì¸ìŠ¤í„´ìŠ¤
        """
        if not DATA_INTERFACE_AVAILABLE:
            raise ImportError("_quantum_data_interface module not available")

        collector = QuantumDataCollector(student_id=student_id)
        features = collector.collect_all(agent_contexts)
        state_8d = DimensionReducer.transform_to_list(features)

        return cls.from_list(state_8d)

    def to_list(self):
        """8D ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return [
            self.cognitive_clarity,
            self.emotional_stability,
            self.engagement_level,
            self.concept_mastery,
            self.routine_strength,
            self.metacognitive_awareness,
            self.dropout_risk,
            self.intervention_readiness
        ]

    def to_dict(self):
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'cognitive_clarity': self.cognitive_clarity,
            'emotional_stability': self.emotional_stability,
            'engagement_level': self.engagement_level,
            'concept_mastery': self.concept_mastery,
            'routine_strength': self.routine_strength,
            'metacognitive_awareness': self.metacognitive_awareness,
            'dropout_risk': self.dropout_risk,
            'intervention_readiness': self.intervention_readiness
        }

    def __repr__(self):
        return f"New8DStateVector({', '.join(f'{k}={v:.3f}' for k, v in self.to_dict().items())})"


def convert_new8d_to_old8d(new_state):
    """
    New 8D StateVector â†’ Old 8D StateVector ë³€í™˜

    ë§¤í•‘ ê·œì¹™:
    - cognitive_clarity â†’ metacognition
    - emotional_stability â†’ emotional_regulation
    - engagement_level â†’ engagement
    - concept_mastery â†’ self_efficacy
    - routine_strength â†’ (confidence + motivation) / 2
    - metacognitive_awareness â†’ metacognition (ë³´ì •)
    - dropout_risk â†’ 1 - motivation
    - intervention_readiness â†’ help_seeking
    """
    if isinstance(new_state, New8DStateVector):
        d = new_state.to_dict()
    elif isinstance(new_state, list) and len(new_state) == 8:
        d = dict(zip(NEW_8D_DIMENSIONS, new_state))
    else:
        raise ValueError("Invalid new state format")

    # ë§¤í•‘ (ê·¼ì‚¬ì  ë³€í™˜)
    metacognition = (d['cognitive_clarity'] + d['metacognitive_awareness']) / 2
    self_efficacy = d['concept_mastery']
    help_seeking = d['intervention_readiness']
    emotional_regulation = d['emotional_stability']
    anxiety = d['dropout_risk']  # ì´íƒˆ ìœ„í—˜ â‰ˆ ë¶ˆì•ˆ
    confidence = d['routine_strength']
    engagement = d['engagement_level']
    motivation = 1.0 - d['dropout_risk']

    return StateVector(
        metacognition=metacognition,
        self_efficacy=self_efficacy,
        help_seeking=help_seeking,
        emotional_regulation=emotional_regulation,
        anxiety=anxiety,
        confidence=confidence,
        engagement=engagement,
        motivation=motivation
    )


def convert_old8d_to_new8d(old_state):
    """
    Old 8D StateVector â†’ New 8D StateVector ë³€í™˜

    ì—­ë§¤í•‘ ê·œì¹™
    """
    if isinstance(old_state, StateVector):
        d = old_state.to_dict()
    else:
        raise ValueError("Invalid old state format")

    cognitive_clarity = d['metacognition']
    emotional_stability = d['emotional_regulation']
    engagement_level = d['engagement']
    concept_mastery = d['self_efficacy']
    routine_strength = d['confidence']
    metacognitive_awareness = d['metacognition']
    dropout_risk = d['anxiety']
    intervention_readiness = d['help_seeking']

    return New8DStateVector(
        cognitive_clarity=cognitive_clarity,
        emotional_stability=emotional_stability,
        engagement_level=engagement_level,
        concept_mastery=concept_mastery,
        routine_strength=routine_strength,
        metacognitive_awareness=metacognitive_awareness,
        dropout_risk=dropout_risk,
        intervention_readiness=intervention_readiness
    )


def calculate_new8d_similarity(state1, state2):
    """
    ë‘ New 8D StateVector ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°

    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
    """
    if isinstance(state1, New8DStateVector):
        v1 = state1.to_list()
    else:
        v1 = state1

    if isinstance(state2, New8DStateVector):
        v2 = state2.to_list()
    else:
        v2 = state2

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    dot_product = sum(a * b for a, b in zip(v1, v2))
    norm1 = math.sqrt(sum(a * a for a in v1))
    norm2 = math.sqrt(sum(b * b for b in v2))

    if norm1 == 0 or norm2 == 0:
        return 0.0

    return dot_product / (norm1 * norm2)


# ì—ì´ì „íŠ¸ë³„ ìµœì  New 8D StateVector ì •ì˜
AGENT_OPTIMAL_NEW8D = {
    1: New8DStateVector(  # Onboarding - ìƒˆë¡œìš´ ì‹œì‘
        cognitive_clarity=0.50, emotional_stability=0.60, engagement_level=0.70,
        concept_mastery=0.50, routine_strength=0.50, metacognitive_awareness=0.50,
        dropout_risk=0.30, intervention_readiness=0.70
    ),
    3: New8DStateVector(  # Goals Analysis - ëª©í‘œ ì„¤ì •
        cognitive_clarity=0.70, emotional_stability=0.65, engagement_level=0.75,
        concept_mastery=0.60, routine_strength=0.60, metacognitive_awareness=0.70,
        dropout_risk=0.25, intervention_readiness=0.65
    ),
    5: New8DStateVector(  # Learning Emotion - ì •ì„œ ì§€ì›
        cognitive_clarity=0.50, emotional_stability=0.40, engagement_level=0.50,
        concept_mastery=0.40, routine_strength=0.40, metacognitive_awareness=0.50,
        dropout_risk=0.60, intervention_readiness=0.80
    ),
    8: New8DStateVector(  # Calmness - ì§„ì • ìœ ë„
        cognitive_clarity=0.55, emotional_stability=0.35, engagement_level=0.55,
        concept_mastery=0.45, routine_strength=0.40, metacognitive_awareness=0.55,
        dropout_risk=0.70, intervention_readiness=0.75
    ),
    9: New8DStateVector(  # Pomodoro - ì‹œê°„ ê´€ë¦¬
        cognitive_clarity=0.60, emotional_stability=0.60, engagement_level=0.70,
        concept_mastery=0.55, routine_strength=0.75, metacognitive_awareness=0.60,
        dropout_risk=0.35, intervention_readiness=0.60
    ),
    10: New8DStateVector(  # Concept Notes - ê°œë… í•™ìŠµ
        cognitive_clarity=0.70, emotional_stability=0.65, engagement_level=0.80,
        concept_mastery=0.55, routine_strength=0.55, metacognitive_awareness=0.70,
        dropout_risk=0.30, intervention_readiness=0.65
    ),
    11: New8DStateVector(  # Problem Notes - ë¬¸ì œ í’€ì´
        cognitive_clarity=0.65, emotional_stability=0.60, engagement_level=0.80,
        concept_mastery=0.60, routine_strength=0.55, metacognitive_awareness=0.65,
        dropout_risk=0.35, intervention_readiness=0.60
    ),
    12: New8DStateVector(  # Rest Routine - íœ´ì‹
        cognitive_clarity=0.50, emotional_stability=0.50, engagement_level=0.35,
        concept_mastery=0.45, routine_strength=0.45, metacognitive_awareness=0.50,
        dropout_risk=0.50, intervention_readiness=0.50
    ),
    4: New8DStateVector(  # Engagement - ì°¸ì—¬ ì´‰ì§„
        cognitive_clarity=0.60, emotional_stability=0.60, engagement_level=0.75,
        concept_mastery=0.55, routine_strength=0.60, metacognitive_awareness=0.60,
        dropout_risk=0.40, intervention_readiness=0.70
    ),
}

# ==============================================================================
# 1. ìš´ì˜ ëª¨ë“œ ì •ì˜
# ==============================================================================

class OrchestratorMode(Enum):
    """Orchestrator ìš´ì˜ ëª¨ë“œ"""
    OBSERVE = "observe"       # ê´€ì°° ëª¨ë“œ: ë¡œê¹…ë§Œ, ê¸°ì¡´ ì‹œìŠ¤í…œ ì˜í–¥ ì—†ìŒ
    COMPARE = "compare"       # ë¹„êµ ëª¨ë“œ: ì œì•ˆ vs ì‹¤ì œ ê²°ê³¼ ë¹„êµ
    SUGGEST = "suggest"       # ì œì•ˆ ëª¨ë“œ: ìµœì  ìˆœì„œ ì œì•ˆ
    ACTIVE = "active"         # í™œì„± ëª¨ë“œ: ì‹¤ì œ ìˆœì„œ ì¡°ì • (ë¯¸ë˜ìš©)


# ==============================================================================
# 2. ì—ì´ì „íŠ¸ ì‹ í˜¸ êµ¬ì¡°
# ==============================================================================

class AgentSignal:
    """ì—ì´ì „íŠ¸ì˜ Quantum ì‹ í˜¸ (Python 3.6 í˜¸í™˜)"""

    def __init__(self, agent_id, amplitude, phase, confidence, rule_id=None):
        # type: (int, float, float, float, Optional[str]) -> None
        self.agent_id = agent_id
        self.amplitude = amplitude        # 0.0 ~ 1.0
        self.phase = phase                # ë¼ë””ì•ˆ
        self.confidence = confidence      # ì›ë³¸ ë£° confidence
        self.rule_id = rule_id

    @property
    def complex_value(self):
        # type: () -> complex
        """ë³µì†Œìˆ˜ í‘œí˜„"""
        return self.amplitude * complex(math.cos(self.phase), math.sin(self.phase))


class AgentPriority:
    """ì—ì´ì „íŠ¸ í™œì„±í™” ìš°ì„ ìˆœìœ„ (Python 3.6 í˜¸í™˜)"""

    def __init__(self, agent_id, priority_score, entanglement_bonus, state_alignment, signal_strength):
        # type: (int, float, float, float, float) -> None
        self.agent_id = agent_id
        self.priority_score = priority_score      # ë†’ì„ìˆ˜ë¡ ë¨¼ì € í™œì„±í™”
        self.entanglement_bonus = entanglement_bonus  # ì–½í˜ ë³´ë„ˆìŠ¤
        self.state_alignment = state_alignment    # í•™ìƒ ìƒíƒœ ì •ë ¬ë„
        self.signal_strength = signal_strength    # ì‹ í˜¸ ê°•ë„

    def __lt__(self, other):
        return self.priority_score < other.priority_score


# ==============================================================================
# 3. ì‹œë‚˜ë¦¬ì˜¤ â†’ ìœ„ìƒ(Phase) ë§¤í•‘
# ==============================================================================

SCENARIO_PHASE_MAP = {
    "S0": 0.0,                    # ì •ë³´ ìˆ˜ì§‘ (0Â°)
    "S1": math.pi / 4,            # ëª©í‘œ-ê³„íš ë¶ˆì¼ì¹˜ (45Â°)
    "S2": math.pi / 2,            # ì‹œê°„ ë¶€ì¡± ë”œë ˆë§ˆ (90Â°)
    "S3": 3 * math.pi / 4,        # íšŒë³µíƒ„ë ¥ì„± (135Â°)
    "S4": math.pi,                # ì»¤ë¦¬í˜ëŸ¼ ì •í•©ì„± (180Â°)
    "S5": 5 * math.pi / 4,        # ì¥ê¸° ëª©í‘œ (225Â°)
    "C": 3 * math.pi / 2,         # ë³µí•© ìƒí™© (270Â°)
    "Q": 7 * math.pi / 4,         # í¬ê´„í˜• ì§ˆë¬¸ (315Â°)
    "E": math.pi / 6,             # ì •ì„œì  UX (30Â°)
}


# ==============================================================================
# 4. ì—ì´ì „íŠ¸ë³„ ìµœì  StateVector ì •ì˜
# ==============================================================================

# ê° ì—ì´ì „íŠ¸ê°€ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ ê°œì…í•  ìˆ˜ ìˆëŠ” í•™ìƒ ìƒíƒœ
AGENT_OPTIMAL_STATES = {
    1: StateVector(  # Onboarding - ìƒˆë¡œìš´ ì‹œì‘, ì¤‘ê°„ ìˆ˜ì¤€ ìƒíƒœ
        metacognition=0.50, self_efficacy=0.50, help_seeking=0.60,
        emotional_regulation=0.60, anxiety=0.40, confidence=0.50,
        engagement=0.70, motivation=0.70
    ),
    3: StateVector(  # Goals Analysis - ë†’ì€ ë©”íƒ€ì¸ì§€ í•„ìš”
        metacognition=0.70, self_efficacy=0.60, help_seeking=0.65,
        emotional_regulation=0.65, anxiety=0.35, confidence=0.60,
        engagement=0.75, motivation=0.80
    ),
    5: StateVector(  # Learning Emotion - ì •ì„œì  ì§€ì› í•„ìš”
        metacognition=0.50, self_efficacy=0.40, help_seeking=0.70,
        emotional_regulation=0.40, anxiety=0.60, confidence=0.35,
        engagement=0.50, motivation=0.50
    ),
    8: StateVector(  # Calmness - ë†’ì€ ë¶ˆì•ˆ, ë‚®ì€ ì •ì„œì¡°ì ˆ
        metacognition=0.55, self_efficacy=0.45, help_seeking=0.60,
        emotional_regulation=0.35, anxiety=0.75, confidence=0.40,
        engagement=0.55, motivation=0.50
    ),
    10: StateVector(  # Concept Notes - ê°œë… ì´í•´ ì§‘ì¤‘
        metacognition=0.70, self_efficacy=0.55, help_seeking=0.65,
        emotional_regulation=0.65, anxiety=0.35, confidence=0.55,
        engagement=0.80, motivation=0.75
    ),
    11: StateVector(  # Problem Notes - ë¬¸ì œ í’€ì´ ì§‘ì¤‘
        metacognition=0.65, self_efficacy=0.60, help_seeking=0.60,
        emotional_regulation=0.60, anxiety=0.40, confidence=0.55,
        engagement=0.80, motivation=0.70
    ),
    12: StateVector(  # Rest Routine - í”¼ë¡œ ëˆ„ì , íœ´ì‹ í•„ìš”
        metacognition=0.50, self_efficacy=0.45, help_seeking=0.50,
        emotional_regulation=0.50, anxiety=0.50, confidence=0.45,
        engagement=0.35, motivation=0.40
    ),
}


# ==============================================================================
# 5. Hamiltonian Evolution (Flow State ìµœì í™”)
# ==============================================================================

class HamiltonianEvolution:
    """
    Hamiltonian ì§„í™” ì—°ì‚°ì (Python 3.6 í˜¸í™˜)

    ëª©ì : í•™ìƒ ìƒíƒœë¥¼ Flow Stateë¡œ ì ì§„ì  ì§„í™”
    Flow State ì •ì˜: ë†’ì€ ì°¸ì—¬ë„, ì ì ˆí•œ ë„ì „, ë‚®ì€ ë¶ˆì•ˆ
    """

    def __init__(self, target_flow_state=None, evolution_rate=0.1, max_iterations=100):
        # type: (Optional[StateVector], float, int) -> None
        if target_flow_state is None:
            target_flow_state = StateVector(
                metacognition=0.75,
                self_efficacy=0.70,
                help_seeking=0.60,
                emotional_regulation=0.75,
                anxiety=0.25,
                confidence=0.70,
                engagement=0.85,
                motivation=0.80
            )
        self.target_flow_state = target_flow_state
        self.evolution_rate = evolution_rate  # ì§„í™” ì†ë„ (0.0 ~ 1.0)
        self.max_iterations = max_iterations

    def evolve_step(self, current):
        """í•œ ë‹¨ê³„ ì§„í™”"""
        current_dict = current.to_dict()
        target_dict = self.target_flow_state.to_dict()

        evolved = {}
        for key in current_dict:
            diff = target_dict[key] - current_dict[key]
            evolved[key] = current_dict[key] + diff * self.evolution_rate
            # 0.0 ~ 1.0 ë²”ìœ„ ì œí•œ
            evolved[key] = max(0.0, min(1.0, evolved[key]))

        return StateVector(**evolved)

    def evolve_to_flow(
        self,
        initial: StateVector,
        threshold: float = 0.95
    ) -> Tuple[StateVector, int]:
        """
        Flow Stateë¡œ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ì§„í™”

        Returns: (ìµœì¢… ìƒíƒœ, ë°˜ë³µ íšŸìˆ˜)
        """
        current = initial
        for i in range(self.max_iterations):
            similarity = calculate_similarity(current, self.target_flow_state)
            if similarity >= threshold:
                return current, i
            current = self.evolve_step(current)

        return current, self.max_iterations

    def calculate_flow_distance(self, state: StateVector) -> float:
        """í˜„ì¬ ìƒíƒœì™€ Flow State ê°„ì˜ ê±°ë¦¬ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)"""
        return 1.0 - calculate_similarity(state, self.target_flow_state)


# ==============================================================================
# 6. ê°„ì„­ ê³„ì‚°ê¸°
# ==============================================================================

class InterferenceCalculator:
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ Quantum ê°„ì„­ ê³„ì‚°"""

    def __init__(self):
        self.entanglement_map = ENTANGLEMENT_MAP

    def calculate_interference(
        self,
        signals: List[AgentSignal]
    ) -> Tuple[float, Dict[int, float]]:
        """
        ì—¬ëŸ¬ ì‹ í˜¸ì˜ ê°„ì„­ ê³„ì‚°

        Returns: (ì´ amplitude, ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„)
        """
        if not signals:
            return 0.0, {}

        # ë³µì†Œìˆ˜ í•©
        total = complex(0, 0)
        contributions = {}

        for signal in signals:
            cv = signal.complex_value
            total += cv
            contributions[signal.agent_id] = abs(cv)

        total_amplitude = abs(total)

        # ê¸°ì—¬ë„ ì •ê·œí™”
        if total_amplitude > 0:
            for aid in contributions:
                contributions[aid] /= total_amplitude

        return total_amplitude, contributions

    def calculate_entanglement_boost(
        self,
        active_agents: List[int]
    ) -> Dict[int, float]:
        """
        í™œì„±í™”ëœ ì—ì´ì „íŠ¸ë“¤ ê°„ì˜ ì–½í˜ ë¶€ìŠ¤íŠ¸ ê³„ì‚°

        ë†’ì€ ìƒê´€ê´€ê³„ì˜ ì—ì´ì „íŠ¸ë“¤ì´ í•¨ê»˜ í™œì„±í™”ë˜ë©´ ë¶€ìŠ¤íŠ¸
        """
        boosts = defaultdict(float)

        for i, agent1 in enumerate(active_agents):
            for agent2 in active_agents[i+1:]:
                corr = get_correlation(agent1, agent2)
                if corr > 0:
                    boosts[agent1] += corr * 0.5
                    boosts[agent2] += corr * 0.5

        return dict(boosts)


# ==============================================================================
# 7. Quantum Orchestrator ë©”ì¸ í´ë˜ìŠ¤
# ==============================================================================

class QuantumOrchestrator:
    """
    ê¸°ì¡´ ë£° ì‹œìŠ¤í…œ ìœ„ì˜ Quantum ì¡°ì • ë ˆì´ì–´

    ì—­í• :
    1. ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ë™ì‹œ íŠ¸ë¦¬ê±° ì‹œ ìµœì  ìˆœì„œ ì œì•ˆ
    2. í•™ìƒ ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì í•©ë„ í‰ê°€
    3. ì—ì´ì „íŠ¸ ê°„ ì–½í˜ ê³ ë ¤í•œ ì‹œë„ˆì§€ ìµœì í™”
    """

    def __init__(self, mode: OrchestratorMode = OrchestratorMode.OBSERVE):
        self.mode = mode
        self.interference = InterferenceCalculator()
        self.evolution = HamiltonianEvolution()
        self.history: List[Dict] = []  # ê²°ì • íˆìŠ¤í† ë¦¬

    def _calculate_state_alignment(
        self,
        agent_id: int,
        student_state: StateVector
    ) -> float:
        """ì—ì´ì „íŠ¸ì™€ í•™ìƒ ìƒíƒœì˜ ì •ë ¬ë„ ê³„ì‚°"""
        if agent_id not in AGENT_OPTIMAL_STATES:
            return 0.5  # ê¸°ë³¸ê°’

        optimal = AGENT_OPTIMAL_STATES[agent_id]
        similarity = calculate_similarity(student_state, optimal)

        # -1 ~ 1 ë²”ìœ„ë¥¼ 0 ~ 1ë¡œ ë³€í™˜
        return (similarity + 1) / 2

    def _create_signal(
        self,
        agent_id: int,
        priority: int = 90,
        confidence: float = 0.9,
        scenario: str = "S0"
    ) -> AgentSignal:
        """ë£° ì •ë³´ì—ì„œ ì—ì´ì „íŠ¸ ì‹ í˜¸ ìƒì„±"""
        amplitude = confidence * math.sqrt(priority / 100)
        phase = SCENARIO_PHASE_MAP.get(scenario, 0.0)

        return AgentSignal(
            agent_id=agent_id,
            amplitude=amplitude,
            phase=phase,
            confidence=confidence
        )

    def suggest_agent_order(
        self,
        student_state: StateVector,
        triggered_agents: List[int],
        agent_priorities: Optional[Dict[int, int]] = None,
        agent_confidences: Optional[Dict[int, float]] = None,
        agent_scenarios: Optional[Dict[int, str]] = None
    ) -> List[AgentPriority]:
        """
        ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ë™ì‹œ íŠ¸ë¦¬ê±° ì‹œ ìµœì  ìˆœì„œ ì œì•ˆ

        Args:
            student_state: í˜„ì¬ í•™ìƒ ì‹¬ë¦¬ ìƒíƒœ
            triggered_agents: íŠ¸ë¦¬ê±°ëœ ì—ì´ì „íŠ¸ ID ëª©ë¡
            agent_priorities: ì—ì´ì „íŠ¸ë³„ priority (ê¸°ë³¸ 90)
            agent_confidences: ì—ì´ì „íŠ¸ë³„ confidence (ê¸°ë³¸ 0.9)
            agent_scenarios: ì—ì´ì „íŠ¸ë³„ scenario (ê¸°ë³¸ S0)

        Returns: ìš°ì„ ìˆœìœ„ ì •ë ¬ëœ AgentPriority ëª©ë¡
        """
        if not triggered_agents:
            return []

        # ê¸°ë³¸ê°’ ì„¤ì •
        priorities = agent_priorities or {}
        confidences = agent_confidences or {}
        scenarios = agent_scenarios or {}

        # ê° ì—ì´ì „íŠ¸ì˜ ì‹ í˜¸ ìƒì„±
        signals = []
        for aid in triggered_agents:
            signal = self._create_signal(
                agent_id=aid,
                priority=priorities.get(aid, 90),
                confidence=confidences.get(aid, 0.9),
                scenario=scenarios.get(aid, "S0")
            )
            signals.append(signal)

        # ê°„ì„­ ê³„ì‚°
        total_amp, contributions = self.interference.calculate_interference(signals)

        # ì–½í˜ ë¶€ìŠ¤íŠ¸ ê³„ì‚°
        entanglement_boosts = self.interference.calculate_entanglement_boost(
            triggered_agents
        )

        # ê° ì—ì´ì „íŠ¸ì˜ ìµœì¢… ìš°ì„ ìˆœìœ„ ê³„ì‚°
        agent_results = []
        for signal in signals:
            aid = signal.agent_id

            # 1. í•™ìƒ ìƒíƒœ ì •ë ¬ë„
            state_alignment = self._calculate_state_alignment(aid, student_state)

            # 2. ì‹ í˜¸ ê°•ë„
            signal_strength = signal.amplitude

            # 3. ì–½í˜ ë³´ë„ˆìŠ¤
            entanglement_bonus = entanglement_boosts.get(aid, 0.0)

            # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚°
            # ê°€ì¤‘ì¹˜: ìƒíƒœì •ë ¬(40%) + ì‹ í˜¸ê°•ë„(35%) + ì–½í˜ë³´ë„ˆìŠ¤(25%)
            priority_score = (
                state_alignment * 0.40 +
                signal_strength * 0.35 +
                entanglement_bonus * 0.25
            )

            agent_results.append(AgentPriority(
                agent_id=aid,
                priority_score=priority_score,
                entanglement_bonus=entanglement_bonus,
                state_alignment=state_alignment,
                signal_strength=signal_strength
            ))

        # ìš°ì„ ìˆœìœ„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        agent_results.sort(reverse=True)

        # íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self._log_decision(student_state, triggered_agents, agent_results)

        return agent_results

    def _log_decision(
        self,
        state: StateVector,
        agents: List[int],
        result: List[AgentPriority]
    ):
        """ê²°ì • íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        self.history.append({
            "state": state.to_dict(),
            "triggered_agents": agents,
            "suggested_order": [r.agent_id for r in result],
            "scores": {r.agent_id: r.priority_score for r in result}
        })

    def get_flow_optimized_order(
        self,
        student_state: StateVector,
        triggered_agents: List[int]
    ) -> Tuple[List[AgentPriority], StateVector]:
        """
        Flow State ìµœì í™”ëœ ì—ì´ì „íŠ¸ ìˆœì„œ

        í•™ìƒì„ Flow Stateë¡œ ì´ëŒê¸°ì— ê°€ì¥ íš¨ê³¼ì ì¸ ìˆœì„œ ì œì•ˆ

        Returns: (ìš°ì„ ìˆœìœ„ ëª©ë¡, ì˜ˆìƒ ìµœì¢… ìƒíƒœ)
        """
        # í˜„ì¬ ìƒíƒœì—ì„œ Flow Stateê¹Œì§€ì˜ ê±°ë¦¬
        initial_distance = self.evolution.calculate_flow_distance(student_state)

        # ê¸°ë³¸ ìˆœì„œ ê³„ì‚°
        base_order = self.suggest_agent_order(student_state, triggered_agents)

        # Flow Stateë¡œ ì§„í™” ì‹œë®¬ë ˆì´ì…˜
        evolved_state, iterations = self.evolution.evolve_to_flow(student_state)

        # Flow ê¸°ì—¬ë„ë¡œ ì¬ì •ë ¬
        flow_adjusted = []
        for ap in base_order:
            # Flow Stateì— ë” ê°€ê¹ê²Œ ë§Œë“œëŠ” ì—ì´ì „íŠ¸ ìš°ëŒ€
            if ap.agent_id in AGENT_OPTIMAL_STATES:
                optimal = AGENT_OPTIMAL_STATES[ap.agent_id]
                flow_sim = calculate_similarity(optimal, self.evolution.target_flow_state)
                flow_bonus = (flow_sim + 1) / 2 * 0.2  # 0 ~ 0.2 ë³´ë„ˆìŠ¤

                flow_adjusted.append(AgentPriority(
                    agent_id=ap.agent_id,
                    priority_score=ap.priority_score + flow_bonus,
                    entanglement_bonus=ap.entanglement_bonus,
                    state_alignment=ap.state_alignment,
                    signal_strength=ap.signal_strength
                ))
            else:
                flow_adjusted.append(ap)

        flow_adjusted.sort(reverse=True)

        return flow_adjusted, evolved_state

    def compare_with_actual(
        self,
        suggested: List[int],
        actual: List[int],
        outcome_score: float
    ) -> Dict:
        """
        ì œì•ˆ ìˆœì„œì™€ ì‹¤ì œ ìˆœì„œ ë¹„êµ (ë¹„êµ ëª¨ë“œìš©)

        Args:
            suggested: ì œì•ˆëœ ì—ì´ì „íŠ¸ ìˆœì„œ
            actual: ì‹¤ì œ ì‹¤í–‰ëœ ìˆœì„œ
            outcome_score: ì‹¤ì œ ê²°ê³¼ ì ìˆ˜ (0.0 ~ 1.0)

        Returns: ë¹„êµ ë¶„ì„ ê²°ê³¼
        """
        # ìˆœì„œ ì¼ì¹˜ë„ ê³„ì‚° (Kendall tau ê°„ì†Œí™” ë²„ì „)
        matches = sum(1 for i, (s, a) in enumerate(zip(suggested, actual)) if s == a)
        order_similarity = matches / max(len(suggested), len(actual)) if suggested and actual else 0

        return {
            "suggested_order": suggested,
            "actual_order": actual,
            "order_similarity": order_similarity,
            "outcome_score": outcome_score,
            "suggestion_quality": order_similarity * outcome_score,
            "could_improve": order_similarity < 0.8 and outcome_score < 0.7
        }

    # ==========================================================================
    # Phase 8: New 8D StateVector í†µí•© ë©”ì„œë“œ
    # ==========================================================================

    def _calculate_state_alignment_new8d(
        self,
        agent_id: int,
        student_state: 'New8DStateVector'
    ) -> float:
        """New 8D ì—ì´ì „íŠ¸ì™€ í•™ìƒ ìƒíƒœì˜ ì •ë ¬ë„ ê³„ì‚°"""
        if agent_id not in AGENT_OPTIMAL_NEW8D:
            return 0.5  # ê¸°ë³¸ê°’

        optimal = AGENT_OPTIMAL_NEW8D[agent_id]
        similarity = calculate_new8d_similarity(student_state, optimal)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ì´ë¯¸ 0~1 ë²”ìœ„ (ì •ê·œí™”ëœ ë²¡í„°ì˜ ê²½ìš°)
        return similarity

    def suggest_agent_order_from_new8d(
        self,
        student_state: 'New8DStateVector',
        triggered_agents: List[int],
        agent_priorities: Optional[Dict[int, int]] = None,
        agent_confidences: Optional[Dict[int, float]] = None,
        agent_scenarios: Optional[Dict[int, str]] = None
    ) -> List[AgentPriority]:
        """
        New 8D StateVector ê¸°ë°˜ ìµœì  ì—ì´ì „íŠ¸ ìˆœì„œ ì œì•ˆ

        Phase 7 ë°ì´í„° ì¸í„°í˜ì´ìŠ¤ì˜ 8D ë²¡í„°ë¥¼ ì§ì ‘ ì‚¬ìš©

        Args:
            student_state: New 8D í•™ìƒ ìƒíƒœ (New8DStateVector)
            triggered_agents: íŠ¸ë¦¬ê±°ëœ ì—ì´ì „íŠ¸ ID ëª©ë¡
            agent_priorities: ì—ì´ì „íŠ¸ë³„ priority (ê¸°ë³¸ 90)
            agent_confidences: ì—ì´ì „íŠ¸ë³„ confidence (ê¸°ë³¸ 0.9)
            agent_scenarios: ì—ì´ì „íŠ¸ë³„ scenario (ê¸°ë³¸ S0)

        Returns: ìš°ì„ ìˆœìœ„ ì •ë ¬ëœ AgentPriority ëª©ë¡
        """
        if not triggered_agents:
            return []

        # ê¸°ë³¸ê°’ ì„¤ì •
        priorities = agent_priorities or {}
        confidences = agent_confidences or {}
        scenarios = agent_scenarios or {}

        # ê° ì—ì´ì „íŠ¸ì˜ ì‹ í˜¸ ìƒì„±
        signals = []
        for aid in triggered_agents:
            signal = self._create_signal(
                agent_id=aid,
                priority=priorities.get(aid, 90),
                confidence=confidences.get(aid, 0.9),
                scenario=scenarios.get(aid, "S0")
            )
            signals.append(signal)

        # ê°„ì„­ ê³„ì‚°
        total_amp, contributions = self.interference.calculate_interference(signals)

        # ì–½í˜ ë¶€ìŠ¤íŠ¸ ê³„ì‚°
        entanglement_boosts = self.interference.calculate_entanglement_boost(
            triggered_agents
        )

        # ê° ì—ì´ì „íŠ¸ì˜ ìµœì¢… ìš°ì„ ìˆœìœ„ ê³„ì‚°
        agent_results = []
        for signal in signals:
            aid = signal.agent_id

            # 1. New 8D í•™ìƒ ìƒíƒœ ì •ë ¬ë„
            state_alignment = self._calculate_state_alignment_new8d(aid, student_state)

            # 2. ì‹ í˜¸ ê°•ë„
            signal_strength = signal.amplitude

            # 3. ì–½í˜ ë³´ë„ˆìŠ¤
            entanglement_bonus = entanglement_boosts.get(aid, 0.0)

            # 4. dropout_risk ê¸°ë°˜ ì¶”ê°€ ì¡°ì •
            # ì´íƒˆ ìœ„í—˜ì´ ë†’ìœ¼ë©´ ì •ì„œ ì§€ì› ì—ì´ì „íŠ¸(5, 8) ìš°ëŒ€
            dropout_adjustment = 0.0
            if student_state.dropout_risk > 0.6:
                if aid in [5, 8, 12]:  # Emotion, Calmness, Rest
                    dropout_adjustment = 0.15

            # 5. intervention_readiness ê¸°ë°˜ ì¶”ê°€ ì¡°ì •
            # ê°œì… ì¤€ë¹„ë„ ë†’ìœ¼ë©´ ë” ì ê·¹ì ì¸ ì—ì´ì „íŠ¸ ìš°ëŒ€
            intervention_adjustment = 0.0
            if student_state.intervention_readiness > 0.7:
                if aid in [3, 10, 11]:  # Goals, Concept, Problem
                    intervention_adjustment = 0.10

            # 6. ìµœì¢… ì ìˆ˜ ê³„ì‚°
            # ê°€ì¤‘ì¹˜: ìƒíƒœì •ë ¬(35%) + ì‹ í˜¸ê°•ë„(30%) + ì–½í˜ë³´ë„ˆìŠ¤(20%) + íŠ¹ìˆ˜ì¡°ì •(15%)
            priority_score = (
                state_alignment * 0.35 +
                signal_strength * 0.30 +
                entanglement_bonus * 0.20 +
                dropout_adjustment +
                intervention_adjustment
            )

            agent_results.append(AgentPriority(
                agent_id=aid,
                priority_score=priority_score,
                entanglement_bonus=entanglement_bonus,
                state_alignment=state_alignment,
                signal_strength=signal_strength
            ))

        # ìš°ì„ ìˆœìœ„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        agent_results.sort(reverse=True)

        # íˆìŠ¤í† ë¦¬ ê¸°ë¡ (New 8D í˜•ì‹)
        self._log_decision_new8d(student_state, triggered_agents, agent_results)

        return agent_results

    def _log_decision_new8d(
        self,
        state: 'New8DStateVector',
        agents: List[int],
        result: List[AgentPriority]
    ):
        """New 8D ê²°ì • íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        self.history.append({
            "state_type": "new_8d",
            "state": state.to_dict(),
            "triggered_agents": agents,
            "suggested_order": [r.agent_id for r in result],
            "scores": {r.agent_id: r.priority_score for r in result}
        })

    def suggest_from_agent_data(
        self,
        student_id: int,
        agent_contexts: Dict[int, Dict],
        triggered_agents: List[int],
        agent_priorities: Optional[Dict[int, int]] = None,
        agent_confidences: Optional[Dict[int, float]] = None,
        agent_scenarios: Optional[Dict[int, str]] = None
    ) -> Tuple[List[AgentPriority], 'New8DStateVector']:
        """
        ì—ì´ì „íŠ¸ ë°ì´í„°ì—ì„œ ì§ì ‘ StateVector ìƒì„± ë° ìˆœì„œ ì œì•ˆ

        Phase 7 QuantumDataCollectorì™€ ì™„ì „ í†µí•©

        Args:
            student_id: í•™ìƒ ID
            agent_contexts: ì—ì´ì „íŠ¸ ë°ì´í„° {agent_id: {key: value}}
            triggered_agents: íŠ¸ë¦¬ê±°ëœ ì—ì´ì „íŠ¸ ID ëª©ë¡
            ... (ê¸°íƒ€ íŒŒë¼ë¯¸í„°)

        Returns: (ìš°ì„ ìˆœìœ„ ëª©ë¡, ìƒì„±ëœ New8DStateVector)
        """
        if not DATA_INTERFACE_AVAILABLE:
            raise ImportError("_quantum_data_interface module not available")

        # ì—ì´ì „íŠ¸ ë°ì´í„°ì—ì„œ New 8D StateVector ìƒì„±
        state = New8DStateVector.from_agent_data(student_id, agent_contexts)

        # ìˆœì„œ ì œì•ˆ
        results = self.suggest_agent_order_from_new8d(
            student_state=state,
            triggered_agents=triggered_agents,
            agent_priorities=agent_priorities,
            agent_confidences=agent_confidences,
            agent_scenarios=agent_scenarios
        )

        return results, state

    def get_state_analysis(
        self,
        student_state: 'New8DStateVector'
    ) -> Dict:
        """
        í•™ìƒ ìƒíƒœ ì¢…í•© ë¶„ì„

        Args:
            student_state: New 8D í•™ìƒ ìƒíƒœ

        Returns: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        d = student_state.to_dict()

        # 1. ìœ„í—˜ ìˆ˜ì¤€ í‰ê°€
        risk_level = "low"
        if d['dropout_risk'] > 0.7:
            risk_level = "critical"
        elif d['dropout_risk'] > 0.5:
            risk_level = "medium"
        elif d['dropout_risk'] > 0.3:
            risk_level = "low"
        else:
            risk_level = "minimal"

        # 2. ê°œì… ê¶Œì¥ ìˆ˜ì¤€
        intervention_recommendation = "observe"
        if d['intervention_readiness'] > 0.8:
            intervention_recommendation = "active"
        elif d['intervention_readiness'] > 0.6:
            intervention_recommendation = "suggest"
        elif d['intervention_readiness'] > 0.4:
            intervention_recommendation = "compare"

        # 3. ì£¼ìš” ê°•ì /ì•½ì 
        strengths = []
        weaknesses = []

        for dim, value in d.items():
            if dim == 'dropout_risk':
                if value < 0.3:
                    strengths.append(dim)
                elif value > 0.6:
                    weaknesses.append(dim)
            else:
                if value > 0.7:
                    strengths.append(dim)
                elif value < 0.4:
                    weaknesses.append(dim)

        # 4. ê¶Œì¥ ì—ì´ì „íŠ¸
        recommended_agents = []
        if d['emotional_stability'] < 0.4 or d['dropout_risk'] > 0.6:
            recommended_agents.extend([5, 8])  # Emotion, Calmness
        if d['engagement_level'] < 0.4:
            recommended_agents.extend([4, 9])  # Engagement, Pomodoro
        if d['concept_mastery'] < 0.4:
            recommended_agents.extend([10, 11])  # Concept, Problem
        if d['routine_strength'] < 0.4:
            recommended_agents.extend([9, 12])  # Pomodoro, Rest

        return {
            "state": d,
            "risk_level": risk_level,
            "intervention_recommendation": intervention_recommendation,
            "strengths": strengths,
            "weaknesses": weaknesses,
            "recommended_agents": list(set(recommended_agents)),
            "data_interface_available": DATA_INTERFACE_AVAILABLE
        }


# ==============================================================================
# 8. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ==============================================================================

def run_orchestrator_test():
    """Quantum Orchestrator í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("Quantum Orchestration - Phase 3: Orchestrator í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print()

    # 1. Orchestrator ì´ˆê¸°í™”
    orchestrator = QuantumOrchestrator(mode=OrchestratorMode.SUGGEST)
    print(f"ğŸ“Š [1] Orchestrator ì´ˆê¸°í™”: {orchestrator.mode.value} ëª¨ë“œ")
    print()

    # 2. í…ŒìŠ¤íŠ¸ í•™ìƒ ìƒíƒœ (ë¶ˆì•ˆí•œ í•™ìƒ)
    anxious_student = StateVector(
        metacognition=0.50,
        self_efficacy=0.35,
        help_seeking=0.60,
        emotional_regulation=0.30,
        anxiety=0.80,
        confidence=0.30,
        engagement=0.45,
        motivation=0.50
    )

    print("ğŸ“‹ [2] í…ŒìŠ¤íŠ¸ í•™ìƒ ìƒíƒœ (ë¶ˆì•ˆí•œ í•™ìƒ)")
    for k, v in anxious_student.to_dict().items():
        bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
        print(f"   {k:20s}: {bar} {v:.2f}")
    print()

    # 3. ë™ì‹œ íŠ¸ë¦¬ê±°ëœ ì—ì´ì „íŠ¸
    triggered = [5, 8, 10, 12]  # Emotion, Calmness, Concept, Rest
    print(f"ğŸ¯ [3] ë™ì‹œ íŠ¸ë¦¬ê±°ëœ ì—ì´ì „íŠ¸: {[get_agent_name(a) for a in triggered]}")
    print()

    # 4. ìµœì  ìˆœì„œ ì œì•ˆ
    print("âš¡ [4] ìµœì  í™œì„±í™” ìˆœì„œ ì œì•ˆ")
    results = orchestrator.suggest_agent_order(
        student_state=anxious_student,
        triggered_agents=triggered,
        agent_scenarios={5: "E", 8: "E", 10: "S3", 12: "S2"}
    )

    print("   ìˆœìœ„ | ì—ì´ì „íŠ¸            | ì ìˆ˜   | ìƒíƒœì •ë ¬ | ì‹ í˜¸ê°•ë„ | ì–½í˜ë³´ë„ˆìŠ¤")
    print("   " + "-" * 70)
    for i, r in enumerate(results, 1):
        name = get_agent_name(r.agent_id)
        print(f"   {i:4d} | {name:18s} | {r.priority_score:.4f} | {r.state_alignment:.4f}  | {r.signal_strength:.4f}  | {r.entanglement_bonus:.4f}")
    print()

    # 5. Flow State ìµœì í™”
    print("ğŸŒŠ [5] Flow State ìµœì í™”ëœ ìˆœì„œ")
    flow_results, evolved_state = orchestrator.get_flow_optimized_order(
        student_state=anxious_student,
        triggered_agents=triggered
    )

    print("   Flow ìµœì í™” ìˆœì„œ:")
    for i, r in enumerate(flow_results, 1):
        name = get_agent_name(r.agent_id)
        print(f"   {i}. {name} (ì ìˆ˜: {r.priority_score:.4f})")
    print()

    print("   ì˜ˆìƒ ì§„í™” í›„ ìƒíƒœ:")
    for k, v in evolved_state.to_dict().items():
        bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
        print(f"   {k:20s}: {bar} {v:.2f}")
    print()

    # 6. ë‹¤ë¥¸ í•™ìƒ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸ (ë™ê¸° ë†’ì€ í•™ìƒ)
    motivated_student = StateVector(
        metacognition=0.75,
        self_efficacy=0.70,
        help_seeking=0.65,
        emotional_regulation=0.75,
        anxiety=0.25,
        confidence=0.75,
        engagement=0.85,
        motivation=0.90
    )

    print("ğŸ“‹ [6] ë™ê¸° ë†’ì€ í•™ìƒìœ¼ë¡œ ë™ì¼ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    results2 = orchestrator.suggest_agent_order(
        student_state=motivated_student,
        triggered_agents=triggered
    )

    print("   ìˆœì„œ ë¹„êµ:")
    print(f"   - ë¶ˆì•ˆí•œ í•™ìƒ: {[get_agent_name(r.agent_id) for r in results]}")
    print(f"   - ë™ê¸° ë†’ì€ í•™ìƒ: {[get_agent_name(r.agent_id) for r in results2]}")
    print()

    # 7. ë¹„êµ ëª¨ë“œ ì‹œë®¬ë ˆì´ì…˜
    print("ğŸ“Š [7] ë¹„êµ ëª¨ë“œ ì‹œë®¬ë ˆì´ì…˜")
    suggested_order = [r.agent_id for r in results]
    actual_order = [8, 5, 12, 10]  # ê°€ìƒì˜ ì‹¤ì œ ìˆœì„œ

    comparison = orchestrator.compare_with_actual(
        suggested=suggested_order,
        actual=actual_order,
        outcome_score=0.72
    )

    print(f"   - ì œì•ˆ ìˆœì„œ: {[get_agent_name(a) for a in comparison['suggested_order']]}")
    print(f"   - ì‹¤ì œ ìˆœì„œ: {[get_agent_name(a) for a in comparison['actual_order']]}")
    print(f"   - ìˆœì„œ ìœ ì‚¬ë„: {comparison['order_similarity']:.2%}")
    print(f"   - ê²°ê³¼ ì ìˆ˜: {comparison['outcome_score']:.2%}")
    print(f"   - ì œì•ˆ í’ˆì§ˆ: {comparison['suggestion_quality']:.4f}")
    print(f"   - ê°œì„  í•„ìš”: {'ì˜ˆ' if comparison['could_improve'] else 'ì•„ë‹ˆì˜¤'}")
    print()

    print("=" * 60)
    print("âœ… Phase 3 Orchestrator í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)

    return {
        "anxious_order": [r.agent_id for r in results],
        "motivated_order": [r.agent_id for r in results2],
        "flow_order": [r.agent_id for r in flow_results],
        "history_count": len(orchestrator.history)
    }


# ==============================================================================
# 9. New 8D í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (Phase 8 í†µí•© ê²€ì¦)
# ==============================================================================

def run_new8d_test():
    """Phase 8 New 8D StateVector í†µí•© í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("Phase 8: New 8D StateVector í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print()

    # 0. Data Interface ê°€ìš©ì„± í™•ì¸
    print(f"ğŸ“¦ [0] Data Interface ê°€ìš©ì„±: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if DATA_INTERFACE_AVAILABLE else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    print()

    # 1. New8DStateVector ìƒì„± í…ŒìŠ¤íŠ¸
    print("ğŸ“Š [1] New8DStateVector ì§ì ‘ ìƒì„± í…ŒìŠ¤íŠ¸")

    # ë¶ˆì•ˆí•œ í•™ìƒ (New 8D ê¸°ì¤€)
    anxious_new8d = New8DStateVector(
        cognitive_clarity=0.40,      # ë‚®ì€ ì¸ì§€ ëª…í™•ì„±
        emotional_stability=0.30,    # ë‚®ì€ ì •ì„œ ì•ˆì •ì„±
        engagement_level=0.45,       # ì¤‘ê°„ ì°¸ì—¬
        concept_mastery=0.50,        # ì¤‘ê°„ ê°œë… ìˆ™ë‹¬
        routine_strength=0.35,       # ì•½í•œ ë£¨í‹´
        metacognitive_awareness=0.40,  # ë‚®ì€ ë©”íƒ€ì¸ì§€
        dropout_risk=0.70,           # ë†’ì€ ì´íƒˆ ìœ„í—˜
        intervention_readiness=0.80  # ë†’ì€ ê°œì… ì¤€ë¹„ë„
    )

    print("   ë¶ˆì•ˆí•œ í•™ìƒ (New 8D):")
    for k, v in anxious_new8d.to_dict().items():
        bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
        print(f"   {k:25s}: {bar} {v:.2f}")
    print()

    # 2. Old 8D ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("ğŸ”„ [2] New 8D â†’ Old 8D ë³€í™˜ í…ŒìŠ¤íŠ¸")
    old_8d = convert_new8d_to_old8d(anxious_new8d)
    print("   ë³€í™˜ëœ Old 8D StateVector:")
    for k, v in old_8d.to_dict().items():
        bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
        print(f"   {k:20s}: {bar} {v:.2f}")
    print()

    # 3. ì—­ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("ğŸ”„ [3] Old 8D â†’ New 8D ì—­ë³€í™˜ í…ŒìŠ¤íŠ¸")
    recovered_new8d = convert_old8d_to_new8d(old_8d)
    print("   ì—­ë³€í™˜ëœ New 8D StateVector:")
    for k, v in recovered_new8d.to_dict().items():
        bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
        print(f"   {k:25s}: {bar} {v:.2f}")
    print()

    # 4. Orchestrator ì´ˆê¸°í™” ë° New 8D ìˆœì„œ ì œì•ˆ
    print("âš¡ [4] Orchestrator New 8D ìˆœì„œ ì œì•ˆ í…ŒìŠ¤íŠ¸")
    orchestrator = QuantumOrchestrator(mode=OrchestratorMode.SUGGEST)

    triggered = [5, 8, 10, 12]  # Emotion, Calmness, Concept, Rest
    print(f"   íŠ¸ë¦¬ê±°ëœ ì—ì´ì „íŠ¸: {[get_agent_name(a) for a in triggered]}")

    results = orchestrator.suggest_agent_order_from_new8d(
        student_state=anxious_new8d,
        triggered_agents=triggered,
        agent_scenarios={5: "E", 8: "E", 10: "S3", 12: "S2"}
    )

    print("\n   New 8D ê¸°ë°˜ ìµœì  ìˆœì„œ:")
    print("   ìˆœìœ„ | ì—ì´ì „íŠ¸            | ì ìˆ˜   | ìƒíƒœì •ë ¬ | ì‹ í˜¸ê°•ë„ | ì–½í˜ë³´ë„ˆìŠ¤")
    print("   " + "-" * 70)
    for i, r in enumerate(results, 1):
        name = get_agent_name(r.agent_id)
        print(f"   {i:4d} | {name:18s} | {r.priority_score:.4f} | {r.state_alignment:.4f}  | {r.signal_strength:.4f}  | {r.entanglement_bonus:.4f}")
    print()

    # 5. ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    print("ğŸ“ [5] New 8D ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸")

    # ë™ê¸° ë†’ì€ í•™ìƒ
    motivated_new8d = New8DStateVector(
        cognitive_clarity=0.85,
        emotional_stability=0.80,
        engagement_level=0.90,
        concept_mastery=0.75,
        routine_strength=0.80,
        metacognitive_awareness=0.75,
        dropout_risk=0.15,
        intervention_readiness=0.30
    )

    similarity = calculate_new8d_similarity(anxious_new8d, motivated_new8d)
    print(f"   ë¶ˆì•ˆí•œ í•™ìƒ â†” ë™ê¸° ë†’ì€ í•™ìƒ ìœ ì‚¬ë„: {similarity:.4f}")

    # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„
    self_similarity = calculate_new8d_similarity(anxious_new8d, anxious_new8d)
    print(f"   ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ (ê²€ì¦): {self_similarity:.4f} (expected: 1.0)")
    print()

    # 6. ì—ì´ì „íŠ¸ ìµœì  ìƒíƒœ ë§¤ì¹­ í…ŒìŠ¤íŠ¸
    print("ğŸ¯ [6] ì—ì´ì „íŠ¸ë³„ ìµœì  New 8D ìƒíƒœ ë§¤ì¹­ í…ŒìŠ¤íŠ¸")
    print(f"   ì •ì˜ëœ ìµœì  ìƒíƒœ ì—ì´ì „íŠ¸: {list(AGENT_OPTIMAL_NEW8D.keys())}")

    for agent_id in [8, 10, 12]:  # Calmness, Concept, Rest
        if agent_id in AGENT_OPTIMAL_NEW8D:
            optimal = AGENT_OPTIMAL_NEW8D[agent_id]
            sim_anxious = calculate_new8d_similarity(anxious_new8d, optimal)
            sim_motivated = calculate_new8d_similarity(motivated_new8d, optimal)
            print(f"   Agent {agent_id} ({get_agent_name(agent_id)}): "
                  f"ë¶ˆì•ˆí•œ í•™ìƒ={sim_anxious:.3f}, ë™ê¸° ë†’ì€ í•™ìƒ={sim_motivated:.3f}")
    print()

    # 7. get_state_analysis() í…ŒìŠ¤íŠ¸
    print("ğŸ“ˆ [7] ìƒíƒœ ë¶„ì„ API í…ŒìŠ¤íŠ¸")
    analysis = orchestrator.get_state_analysis(
        student_state=anxious_new8d
    )

    print(f"   ë¶„ì„ ê²°ê³¼:")
    print(f"   - ìœ„í—˜ ìˆ˜ì¤€: {analysis['risk_level']}")
    print(f"   - ê°œì… ê¶Œê³ : {analysis['intervention_recommendation']}")
    print(f"   - í•™ìƒ ìƒíƒœ ì°¨ì›: {len(analysis['state'])}ê°œ")
    print(f"   - ê°•ì : {analysis['strengths']}")
    print(f"   - ì•½ì : {analysis['weaknesses']}")
    print(f"   - ì¶”ì²œ ì—ì´ì „íŠ¸: {[get_agent_name(a) for a in analysis['recommended_agents']]}")
    print(f"   - Data Interface: {'ì‚¬ìš© ê°€ëŠ¥' if analysis['data_interface_available'] else 'ì‚¬ìš© ë¶ˆê°€'}")
    print()

    # 8. from_agent_data() í…ŒìŠ¤íŠ¸ (Data Interface ê°€ìš©ì‹œ)
    if DATA_INTERFACE_AVAILABLE:
        print("ğŸ”— [8] Phase 7 Data Interface ì—°ë™ í…ŒìŠ¤íŠ¸")

        # ì‹œë®¬ë ˆì´ì…˜ ì—ì´ì „íŠ¸ ë°ì´í„°
        agent_contexts = {
            8: {'calm_score': 0.72, 'calmness_level': 3},
            11: {'accuracy_rate': 0.85, 'total_problems': 20},
            12: {'rest_count': 5, 'average_interval': 55},
            3: {'goal_progress': 0.6, 'goal_effectiveness': 0.7},
            9: {'pomodoro_completion': 0.8},
            4: {'engagement_level': 0.75, 'dropout_risk': 0.15}
        }

        print(f"   ì—ì´ì „íŠ¸ ì»¨í…ìŠ¤íŠ¸: {list(agent_contexts.keys())}")

        # from_agent_data í…ŒìŠ¤íŠ¸
        state_from_data = New8DStateVector.from_agent_data(
            student_id=99999,
            agent_contexts=agent_contexts
        )

        print("   Data Interfaceì—ì„œ ìƒì„±ëœ New 8D:")
        for k, v in state_from_data.to_dict().items():
            bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
            print(f"   {k:25s}: {bar} {v:.2f}")
        print()

        # suggest_from_agent_data í…ŒìŠ¤íŠ¸
        print("ğŸš€ [9] suggest_from_agent_data í†µí•© í…ŒìŠ¤íŠ¸")
        results_from_data, generated_state = orchestrator.suggest_from_agent_data(
            student_id=99999,
            agent_contexts=agent_contexts,
            triggered_agents=[8, 10, 12],
            agent_scenarios={8: "E", 10: "S3", 12: "S2"}
        )

        print("   Data Interface ê¸°ë°˜ ìµœì  ìˆœì„œ:")
        for i, r in enumerate(results_from_data, 1):
            name = get_agent_name(r.agent_id)
            print(f"   {i}. {name} (ì ìˆ˜: {r.priority_score:.4f})")
        print()
    else:
        print("âš ï¸ [8] Phase 7 Data Interface ì‚¬ìš© ë¶ˆê°€ - í…ŒìŠ¤íŠ¸ ìƒëµ")
        print("   _quantum_data_interface.py ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print()

    # 10. ê²°ê³¼ ìš”ì•½
    print("=" * 60)
    print("âœ… Phase 8 New 8D í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)

    return {
        "new8d_test": "PASSED",
        "conversion_test": "PASSED",
        "similarity_test": "PASSED",
        "orchestrator_test": "PASSED",
        "data_interface_available": DATA_INTERFACE_AVAILABLE,
        "suggested_order": [r.agent_id for r in results]
    }


# ==============================================================================
# Main
# ==============================================================================

if __name__ == "__main__":
    import sys

    # ëª…ë ¹ì¤„ ì¸ìë¡œ í…ŒìŠ¤íŠ¸ ì„ íƒ
    if len(sys.argv) > 1:
        test_type = sys.argv[1].lower()
        if test_type == "old" or test_type == "phase3":
            run_orchestrator_test()
        elif test_type == "new" or test_type == "phase8":
            run_new8d_test()
        elif test_type == "all":
            print("\n" + "=" * 60)
            print("ğŸ”¬ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
            print("=" * 60 + "\n")

            print(">>> Phase 3 (Old 8D) í…ŒìŠ¤íŠ¸ <<<\n")
            result1 = run_orchestrator_test()

            print("\n\n>>> Phase 8 (New 8D) í…ŒìŠ¤íŠ¸ <<<\n")
            result2 = run_new8d_test()

            print("\n" + "=" * 60)
            print("ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            print("=" * 60)
            print(f"   Phase 3 (Old 8D): âœ… ì™„ë£Œ")
            print(f"   Phase 8 (New 8D): âœ… ì™„ë£Œ")
            print(f"   Data Interface: {'âœ… ì—°ë™ë¨' if result2['data_interface_available'] else 'âš ï¸ ë¯¸ì—°ë™'}")
        else:
            print(f"Unknown test type: {test_type}")
            print("Usage: python _quantum_orchestrator.py [old|new|all]")
    else:
        # ê¸°ë³¸: ìƒˆë¡œìš´ 8D í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        run_new8d_test()
