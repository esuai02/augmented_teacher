#!/usr/bin/env python3
"""
Phase 1 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

ì¸¡ì • í•­ëª©:
1. ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹œê°„
2. ê·œì¹™ ì¶”ì¶œ ì‹œê°„
3. ì¶”ë¡  ì‹¤í–‰ ì‹œê°„
4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
5. ì „ì²´ ì²˜ë¦¬ ì‹œê°„
"""

import time
import json
import sys
from typing import Dict, List
from ontology_loader import OntologyLoader
from inference_engine import InferenceEngine


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""

    def __init__(self, ontology_path: str):
        self.ontology_path = ontology_path
        self.results = {}

    def measure_ontology_load(self, iterations: int = 100) -> Dict[str, float]:
        """ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹œê°„ ì¸¡ì •"""
        print(f"\nğŸ“Š ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹œê°„ ì¸¡ì • ({iterations}íšŒ ë°˜ë³µ)...")

        times = []
        for i in range(iterations):
            loader = OntologyLoader(self.ontology_path)

            start_time = time.perf_counter()
            loader.load()
            end_time = time.perf_counter()

            elapsed = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
            times.append(elapsed)

            if (i + 1) % 20 == 0:
                print(f"  ì§„í–‰: {i + 1}/{iterations}íšŒ")

        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        print(f"  âœ“ í‰ê· : {avg_time:.3f}ms")
        print(f"  âœ“ ìµœì†Œ: {min_time:.3f}ms")
        print(f"  âœ“ ìµœëŒ€: {max_time:.3f}ms")

        return {
            'average': avg_time,
            'min': min_time,
            'max': max_time,
            'iterations': iterations
        }

    def measure_rule_extraction(self, iterations: int = 100) -> Dict[str, float]:
        """ê·œì¹™ ì¶”ì¶œ ì‹œê°„ ì¸¡ì •"""
        print(f"\nğŸ“Š ê·œì¹™ ì¶”ì¶œ ì‹œê°„ ì¸¡ì • ({iterations}íšŒ ë°˜ë³µ)...")

        loader = OntologyLoader(self.ontology_path)
        loader.load()

        times = []
        for i in range(iterations):
            start_time = time.perf_counter()
            rules = loader.extract_rules()
            end_time = time.perf_counter()

            elapsed = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
            times.append(elapsed)

            if (i + 1) % 20 == 0:
                print(f"  ì§„í–‰: {i + 1}/{iterations}íšŒ")

        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        rule_count = len(rules)

        print(f"  âœ“ í‰ê· : {avg_time:.3f}ms")
        print(f"  âœ“ ìµœì†Œ: {min_time:.3f}ms")
        print(f"  âœ“ ìµœëŒ€: {max_time:.3f}ms")
        print(f"  âœ“ ê·œì¹™ ìˆ˜: {rule_count}ê°œ")

        return {
            'average': avg_time,
            'min': min_time,
            'max': max_time,
            'rule_count': rule_count,
            'iterations': iterations
        }

    def measure_inference(self, iterations: int = 1000) -> Dict[str, float]:
        """ì¶”ë¡  ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        print(f"\nğŸ“Š ì¶”ë¡  ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ({iterations}íšŒ ë°˜ë³µ)...")

        engine = InferenceEngine(self.ontology_path)

        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_cases = [
            {'emotion': 'Frustrated'},
            {'emotion': 'Focused'},
            {'emotion': 'Tired'},
            {'emotion': 'Anxious'},
            {'emotion': 'Happy'}
        ]

        all_times = []
        case_times = {case['emotion']: [] for case in test_cases}

        for i in range(iterations):
            for test_case in test_cases:
                start_time = time.perf_counter()
                results = engine.infer(test_case)
                end_time = time.perf_counter()

                elapsed = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
                all_times.append(elapsed)
                case_times[test_case['emotion']].append(elapsed)

            if (i + 1) % 200 == 0:
                print(f"  ì§„í–‰: {i + 1}/{iterations}íšŒ")

        # ì „ì²´ í†µê³„
        avg_time = sum(all_times) / len(all_times)
        min_time = min(all_times)
        max_time = max(all_times)

        print(f"  âœ“ ì „ì²´ í‰ê· : {avg_time:.4f}ms")
        print(f"  âœ“ ì „ì²´ ìµœì†Œ: {min_time:.4f}ms")
        print(f"  âœ“ ì „ì²´ ìµœëŒ€: {max_time:.4f}ms")

        # ê°ì •ë³„ í†µê³„
        print("\n  ê°ì •ë³„ í‰ê·  ì‹œê°„:")
        emotion_stats = {}
        for emotion, times in case_times.items():
            emotion_avg = sum(times) / len(times)
            emotion_stats[emotion] = emotion_avg
            print(f"    {emotion}: {emotion_avg:.4f}ms")

        return {
            'average': avg_time,
            'min': min_time,
            'max': max_time,
            'iterations': iterations * len(test_cases),
            'per_emotion': emotion_stats
        }

    def measure_end_to_end(self, iterations: int = 100) -> Dict[str, float]:
        """E2E ì‹œê°„ ì¸¡ì • (ë¡œë“œ + ì¶”ë¡ )"""
        print(f"\nğŸ“Š E2E ì‹œê°„ ì¸¡ì • ({iterations}íšŒ ë°˜ë³µ)...")

        test_case = {'emotion': 'Frustrated'}
        times = []

        for i in range(iterations):
            start_time = time.perf_counter()

            # ì—”ì§„ ì´ˆê¸°í™” + ì¶”ë¡ 
            engine = InferenceEngine(self.ontology_path)
            results = engine.infer(test_case)

            end_time = time.perf_counter()

            elapsed = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
            times.append(elapsed)

            if (i + 1) % 20 == 0:
                print(f"  ì§„í–‰: {i + 1}/{iterations}íšŒ")

        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        print(f"  âœ“ í‰ê· : {avg_time:.3f}ms")
        print(f"  âœ“ ìµœì†Œ: {min_time:.3f}ms")
        print(f"  âœ“ ìµœëŒ€: {max_time:.3f}ms")

        return {
            'average': avg_time,
            'min': min_time,
            'max': max_time,
            'iterations': iterations
        }

    def measure_memory_usage(self) -> Dict[str, int]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ê·¼ì‚¬ì¹˜)"""
        print("\nğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •...")

        import sys

        # ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        loader = OntologyLoader(self.ontology_path)
        ontology = loader.load()

        # JSON ë¬¸ìì—´ í¬ê¸°
        ontology_json = json.dumps(ontology)
        ontology_size = len(ontology_json.encode('utf-8'))

        # ê·œì¹™ ì¶”ì¶œ
        rules = loader.extract_rules()
        rules_json = json.dumps(rules)
        rules_size = len(rules_json.encode('utf-8'))

        # ê°ì • ì¶”ì¶œ
        emotions = loader.extract_emotions()
        emotions_json = json.dumps(emotions)
        emotions_size = len(emotions_json.encode('utf-8'))

        # ì—”ì§„ ê°ì²´
        engine = InferenceEngine(self.ontology_path)

        total_estimated = ontology_size + rules_size + emotions_size

        print(f"  âœ“ ì˜¨í†¨ë¡œì§€: {ontology_size:,} bytes ({ontology_size/1024:.2f} KB)")
        print(f"  âœ“ ê·œì¹™: {rules_size:,} bytes ({rules_size/1024:.2f} KB)")
        print(f"  âœ“ ê°ì •: {emotions_size:,} bytes ({emotions_size/1024:.2f} KB)")
        print(f"  âœ“ ì´ ì˜ˆìƒ: {total_estimated:,} bytes ({total_estimated/1024:.2f} KB)")

        return {
            'ontology_bytes': ontology_size,
            'rules_bytes': rules_size,
            'emotions_bytes': emotions_size,
            'total_bytes': total_estimated
        }

    def run_all_benchmarks(self) -> Dict:
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("="*60)
        print("ğŸš€ Phase 1 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("="*60)

        results = {}

        # 1. ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        results['ontology_load'] = self.measure_ontology_load()

        # 2. ê·œì¹™ ì¶”ì¶œ
        results['rule_extraction'] = self.measure_rule_extraction()

        # 3. ì¶”ë¡  ì‹¤í–‰
        results['inference'] = self.measure_inference()

        # 4. E2E
        results['end_to_end'] = self.measure_end_to_end()

        # 5. ë©”ëª¨ë¦¬
        results['memory'] = self.measure_memory_usage()

        return results

    def print_summary(self, results: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)

        print("\nğŸ” í•µì‹¬ ì§€í‘œ:")
        print(f"  â€¢ ì˜¨í†¨ë¡œì§€ ë¡œë“œ: {results['ontology_load']['average']:.3f}ms")
        print(f"  â€¢ ê·œì¹™ ì¶”ì¶œ: {results['rule_extraction']['average']:.3f}ms")
        print(f"  â€¢ ì¶”ë¡  ì‹¤í–‰: {results['inference']['average']:.4f}ms")
        print(f"  â€¢ E2E ì²˜ë¦¬: {results['end_to_end']['average']:.3f}ms")
        print(f"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©: {results['memory']['total_bytes']/1024:.2f} KB")

        print("\nâš¡ ì„±ëŠ¥ ë¶„ì„:")
        inference_per_sec = 1000 / results['inference']['average']
        print(f"  â€¢ ì´ˆë‹¹ ì¶”ë¡  íšŸìˆ˜: {inference_per_sec:.0f}íšŒ/ì´ˆ")

        e2e_per_sec = 1000 / results['end_to_end']['average']
        print(f"  â€¢ ì´ˆë‹¹ E2E ì²˜ë¦¬: {e2e_per_sec:.0f}íšŒ/ì´ˆ")

        print("\nâœ… ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
        targets = {
            'E2E < 100ms': results['end_to_end']['average'] < 100,
            'ì¶”ë¡  < 1ms': results['inference']['average'] < 1,
            'ë©”ëª¨ë¦¬ < 1MB': results['memory']['total_bytes'] < 1024 * 1024
        }

        for target, achieved in targets.items():
            status = "âœ…" if achieved else "âŒ"
            print(f"  {status} {target}")

        print("\n" + "="*60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    benchmark = PerformanceBenchmark('01_minimal_ontology.json')
    results = benchmark.run_all_benchmarks()
    benchmark.print_summary(results)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = 'benchmark_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!\n")


if __name__ == "__main__":
    main()
